{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "id": "z4Sqke1tC0en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишіть функцію гіпотези лінійної регресії у векторному вигляді;"
      ],
      "metadata": {
        "id": "rYGdHCkdC8dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/Housing.csv\")\n"
      ],
      "metadata": {
        "id": "siAhPZkCt2cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[[\"area\", \"bedrooms\",\"bathrooms\"]]\n",
        "y = df[\"price\"]\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "X_with_intercept = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "print(X_with_intercept)\n",
        "w = np.zeros(X_with_intercept.shape[1])\n",
        "print(w.shape)\n",
        "learning_rate = 0.01\n",
        "n_epochs = 1000\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y1XR-Tyu-FY",
        "outputId": "73c217d5-cddd-4b55-be84-ebb0b2f621ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.          1.04576555  1.40213123  1.42050672]\n",
            " [ 1.          1.75539685  1.40213123  5.4008469 ]\n",
            " [ 1.          2.2161964   0.04723492  1.42050672]\n",
            " ...\n",
            " [ 1.         -0.70527273 -1.30766139 -0.56966336]\n",
            " [ 1.         -1.03244041  0.04723492 -0.56966336]\n",
            " [ 1.         -0.59928883  0.04723492 -0.56966336]]\n",
            "(4,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4ja4vBgCHvJ"
      },
      "outputs": [],
      "source": [
        "def hypothesis(w, X):\n",
        "  return np.dot(X, w)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyp = hypothesis(w, X_with_intercept)\n"
      ],
      "metadata": {
        "id": "6K8JsKFpEeAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Створіть функцію для обчислення функції втрат у векторному вигляді;"
      ],
      "metadata": {
        "id": "_ZXJp8QUDGV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fuction(w, X, y):\n",
        "    m = len(y)\n",
        "    h = hypothesis(w, X)\n",
        "    cost = (1/(2*m)) * np.sum((h - y)**2)\n",
        "    return cost"
      ],
      "metadata": {
        "id": "c5DgPIWSDJwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_fuction(w, X_with_intercept, y)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAkYPDw_dEBc",
        "outputId": "de6ad4be-287f-4a62-9091-c04943e505e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13106916364659.268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реалізуйте один крок градієнтного спуску;"
      ],
      "metadata": {
        "id": "kVYmJuh9GHL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_step(w, X, y, learning_rate):\n",
        "    m = len(y)\n",
        "    h = hypothesis(w, X)\n",
        "\n",
        "    gradient = (1 / m) * X.T @ (h - y)\n",
        "\n",
        "    w -= learning_rate * gradient\n",
        "    return w\n"
      ],
      "metadata": {
        "id": "Uypu327_GIi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step = gradient_descent_step(w, X_with_intercept, y, learning_rate)\n",
        "print(step)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5J4ToRRdY7s",
        "outputId": "0a513edf-d45e-4799-e963-07e6e15eb4ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[47667.29247706 10007.11126918  6842.47137471  9662.61090575]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Знайдіть найкращі параметри $\\vec{w}$ для датасету прогнозуючу ціну на будинок залежно від площі, кількості ванних кімнат та кількості спалень;"
      ],
      "metadata": {
        "id": "Z_ZZY9-utwlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, weight, learning_rate, n_epochs):\n",
        "\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        loss = loss_fuction(weight, X, y)\n",
        "        w = gradient_descent_step(weight, X, y, learning_rate)\n",
        "        print(f'Epoch {epoch + 1}, Loss: {loss}')\n",
        "        print(f'Optimal weights using Gradient Descent: {w}')\n",
        "    return w"
      ],
      "metadata": {
        "id": "KQ_xjmGe3zV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_w = gradient_descent(X_with_intercept, y, w, learning_rate, n_epochs)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDKzibUt1BkM",
        "outputId": "ffb8c853-55aa-4a08-f68e-92103cf6ea1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 895585104082.9313\n",
            "Optimal weights using Gradient Descent: [4766527.55719312  821953.73579121  300571.85273916  696155.24295148]\n",
            "Epoch 2, Loss: 895585102937.1445\n",
            "Optimal weights using Gradient Descent: [4766529.57409825  821953.97599513  300569.84590989  696157.02439102]\n",
            "Epoch 3, Loss: 895585101808.8269\n",
            "Optimal weights using Gradient Descent: [4766531.57083433  821954.21339693  300567.85209887  696158.79507452]\n",
            "Epoch 4, Loss: 895585100697.6975\n",
            "Optimal weights using Gradient Descent: [4766533.54760305  821954.44802565  300565.87122055  696160.55506618]\n",
            "Epoch 5, Loss: 895585099603.4799\n",
            "Optimal weights using Gradient Descent: [4766535.50460409  821954.67991006  300563.90318995  696162.3044298 ]\n",
            "Epoch 6, Loss: 895585098525.902\n",
            "Optimal weights using Gradient Descent: [4766537.44203511  821954.90907862  300561.94792266  696164.04322883]\n",
            "Epoch 7, Loss: 895585097464.6973\n",
            "Optimal weights using Gradient Descent: [4766539.36009182  821955.13555955  300560.00533483  696165.77152633]\n",
            "Epoch 8, Loss: 895585096419.603\n",
            "Optimal weights using Gradient Descent: [4766541.25896797  821955.35938076  300558.07534316  696167.48938501]\n",
            "Epoch 9, Loss: 895585095390.3608\n",
            "Optimal weights using Gradient Descent: [4766543.13885535  821955.58056991  300556.15786493  696169.19686718]\n",
            "Epoch 10, Loss: 895585094376.7173\n",
            "Optimal weights using Gradient Descent: [4766544.99994387  821955.79915437  300554.25281794  696170.89403482]\n",
            "Epoch 11, Loss: 895585093378.4222\n",
            "Optimal weights using Gradient Descent: [4766546.84242149  821956.01516126  300552.36012055  696172.58094952]\n",
            "Epoch 12, Loss: 895585092395.231\n",
            "Optimal weights using Gradient Descent: [4766548.66647434  821956.22861743  300550.47969168  696174.25767251]\n",
            "Epoch 13, Loss: 895585091426.9023\n",
            "Optimal weights using Gradient Descent: [4766550.47228666  821956.43954944  300548.61145076  696175.92426467]\n",
            "Epoch 14, Loss: 895585090473.1989\n",
            "Optimal weights using Gradient Descent: [4766552.26004086  821956.64798363  300546.75531779  696177.5807865 ]\n",
            "Epoch 15, Loss: 895585089533.8878\n",
            "Optimal weights using Gradient Descent: [4766554.02991751  821956.85394605  300544.91121327  696179.22729818]\n",
            "Epoch 16, Loss: 895585088608.7396\n",
            "Optimal weights using Gradient Descent: [4766555.7820954   821957.05746252  300543.07905826  696180.8638595 ]\n",
            "Epoch 17, Loss: 895585087697.529\n",
            "Optimal weights using Gradient Descent: [4766557.51675151  821957.25855857  300541.25877432  696182.49052991]\n",
            "Epoch 18, Loss: 895585086800.0344\n",
            "Optimal weights using Gradient Descent: [4766559.23406106  821957.45725952  300539.45028355  696184.10736852]\n",
            "Epoch 19, Loss: 895585085916.038\n",
            "Optimal weights using Gradient Descent: [4766560.93419752  821957.65359041  300537.65350856  696185.71443408]\n",
            "Epoch 20, Loss: 895585085045.3256\n",
            "Optimal weights using Gradient Descent: [4766562.61733261  821957.84757606  300535.86837247  696187.31178501]\n",
            "Epoch 21, Loss: 895585084187.6862\n",
            "Optimal weights using Gradient Descent: [4766564.28363634  821958.03924101  300534.09479893  696188.89947935]\n",
            "Epoch 22, Loss: 895585083342.9131\n",
            "Optimal weights using Gradient Descent: [4766565.93327704  821958.2286096   300532.33271208  696190.47757484]\n",
            "Epoch 23, Loss: 895585082510.8025\n",
            "Optimal weights using Gradient Descent: [4766567.56642134  821958.4157059   300530.58203657  696192.04612886]\n",
            "Epoch 24, Loss: 895585081691.1542\n",
            "Optimal weights using Gradient Descent: [4766569.18323419  821958.60055375  300528.84269756  696193.60519845]\n",
            "Epoch 25, Loss: 895585080883.7711\n",
            "Optimal weights using Gradient Descent: [4766570.78387891  821958.78317677  300527.11462069  696195.15484031]\n",
            "Epoch 26, Loss: 895585080088.4597\n",
            "Optimal weights using Gradient Descent: [4766572.36851719  821958.96359833  300525.39773212  696196.69511083]\n",
            "Epoch 27, Loss: 895585079305.0294\n",
            "Optimal weights using Gradient Descent: [4766573.93730908  821959.14184157  300523.69195848  696198.22606604]\n",
            "Epoch 28, Loss: 895585078533.293\n",
            "Optimal weights using Gradient Descent: [4766575.49041305  821959.31792941  300521.9972269   696199.74776165]\n",
            "Epoch 29, Loss: 895585077773.0664\n",
            "Optimal weights using Gradient Descent: [4766577.02798599  821959.49188455  300520.31346499  696201.26025304]\n",
            "Epoch 30, Loss: 895585077024.169\n",
            "Optimal weights using Gradient Descent: [4766578.55018319  821959.66372945  300518.64060083  696202.76359528]\n",
            "Epoch 31, Loss: 895585076286.4219\n",
            "Optimal weights using Gradient Descent: [4766580.05715842  821959.83348635  300516.978563    696204.25784309]\n",
            "Epoch 32, Loss: 895585075559.6501\n",
            "Optimal weights using Gradient Descent: [4766581.5490639   821960.00117728  300515.32728054  696205.74305087]\n",
            "Epoch 33, Loss: 895585074843.6819\n",
            "Optimal weights using Gradient Descent: [4766583.02605033  821960.16682405  300513.68668298  696207.21927273]\n",
            "Epoch 34, Loss: 895585074138.3474\n",
            "Optimal weights using Gradient Descent: [4766584.48826689  821960.33044825  300512.05670028  696208.68656241]\n",
            "Epoch 35, Loss: 895585073443.4805\n",
            "Optimal weights using Gradient Descent: [4766585.93586128  821960.49207125  300510.43726292  696210.14497339]\n",
            "Epoch 36, Loss: 895585072758.917\n",
            "Optimal weights using Gradient Descent: [4766587.36897974  821960.65171424  300508.82830179  696211.59455879]\n",
            "Epoch 37, Loss: 895585072084.4957\n",
            "Optimal weights using Gradient Descent: [4766588.787767    821960.80939816  300507.22974827  696213.03537143]\n",
            "Epoch 38, Loss: 895585071420.0586\n",
            "Optimal weights using Gradient Descent: [4766590.1923664   821960.96514377  300505.64153419  696214.46746383]\n",
            "Epoch 39, Loss: 895585070765.4493\n",
            "Optimal weights using Gradient Descent: [4766591.5829198   821961.11897161  300504.06359182  696215.89088819]\n",
            "Epoch 40, Loss: 895585070120.515\n",
            "Optimal weights using Gradient Descent: [4766592.95956766  821961.27090203  300502.49585391  696217.3056964 ]\n",
            "Epoch 41, Loss: 895585069485.1047\n",
            "Optimal weights using Gradient Descent: [4766594.32244905  821961.42095516  300500.93825363  696218.71194004]\n",
            "Epoch 42, Loss: 895585068859.0702\n",
            "Optimal weights using Gradient Descent: [4766595.67170162  821961.56915095  300499.3907246   696220.1096704 ]\n",
            "Epoch 43, Loss: 895585068242.2656\n",
            "Optimal weights using Gradient Descent: [4766597.00746167  821961.71550913  300497.85320088  696221.49893846]\n",
            "Epoch 44, Loss: 895585067634.5474\n",
            "Optimal weights using Gradient Descent: [4766598.32986412  821961.86004927  300496.32561698  696222.8797949 ]\n",
            "Epoch 45, Loss: 895585067035.7745\n",
            "Optimal weights using Gradient Descent: [4766599.63904254  821962.0027907   300494.80790783  696224.2522901 ]\n",
            "Epoch 46, Loss: 895585066445.8082\n",
            "Optimal weights using Gradient Descent: [4766600.93512918  821962.1437526   300493.3000088   696225.61647413]\n",
            "Epoch 47, Loss: 895585065864.5122\n",
            "Optimal weights using Gradient Descent: [4766602.21825495  821962.28295393  300491.80185568  696226.97239679]\n",
            "Epoch 48, Loss: 895585065291.7517\n",
            "Optimal weights using Gradient Descent: [4766603.48854947  821962.42041349  300490.3133847   696228.32010757]\n",
            "Epoch 49, Loss: 895585064727.3948\n",
            "Optimal weights using Gradient Descent: [4766604.74614104  821962.55614985  300488.83453251  696229.65965566]\n",
            "Epoch 50, Loss: 895585064171.3118\n",
            "Optimal weights using Gradient Descent: [4766605.99115669  821962.69018145  300487.36523617  696230.99108998]\n",
            "Epoch 51, Loss: 895585063623.3749\n",
            "Optimal weights using Gradient Descent: [4766607.22372219  821962.8225265   300485.90543317  696232.31445914]\n",
            "Epoch 52, Loss: 895585063083.458\n",
            "Optimal weights using Gradient Descent: [4766608.44396203  821962.95320305  300484.45506141  696233.62981149]\n",
            "Epoch 53, Loss: 895585062551.4374\n",
            "Optimal weights using Gradient Descent: [4766609.65199948  821963.08222897  300483.0140592   696234.93719506]\n",
            "Epoch 54, Loss: 895585062027.1921\n",
            "Optimal weights using Gradient Descent: [4766610.84795654  821963.20962196  300481.58236526  696236.23665762]\n",
            "Epoch 55, Loss: 895585061510.6018\n",
            "Optimal weights using Gradient Descent: [4766612.03195404  821963.33539952  300480.15991872  696237.52824665]\n",
            "Epoch 56, Loss: 895585061001.5488\n",
            "Optimal weights using Gradient Descent: [4766613.20411157  821963.459579    300478.74665912  696238.81200936]\n",
            "Epoch 57, Loss: 895585060499.9177\n",
            "Optimal weights using Gradient Descent: [4766614.36454752  821963.58217756  300477.34252638  696240.08799266]\n",
            "Epoch 58, Loss: 895585060005.5941\n",
            "Optimal weights using Gradient Descent: [4766615.51337911  821963.7032122   300475.94746083  696241.3562432 ]\n",
            "Epoch 59, Loss: 895585059518.4658\n",
            "Optimal weights using Gradient Descent: [4766616.65072238  821963.82269975  300474.56140321  696242.61680734]\n",
            "Epoch 60, Loss: 895585059038.4232\n",
            "Optimal weights using Gradient Descent: [4766617.77669222  821963.94065686  300473.18429463  696243.86973119]\n",
            "Epoch 61, Loss: 895585058565.357\n",
            "Optimal weights using Gradient Descent: [4766618.89140236  821964.05710003  300471.8160766   696245.11506056]\n",
            "Epoch 62, Loss: 895585058099.1609\n",
            "Optimal weights using Gradient Descent: [4766619.9949654   821964.17204558  300470.45669103  696246.35284101]\n",
            "Epoch 63, Loss: 895585057639.73\n",
            "Optimal weights using Gradient Descent: [4766621.08749281  821964.28550968  300469.10608019  696247.58311781]\n",
            "Epoch 64, Loss: 895585057186.9606\n",
            "Optimal weights using Gradient Descent: [4766622.16909495  821964.39750834  300467.76418676  696248.80593598]\n",
            "Epoch 65, Loss: 895585056740.751\n",
            "Optimal weights using Gradient Descent: [4766623.23988106  821964.50805739  300466.43095377  696250.02134026]\n",
            "Epoch 66, Loss: 895585056301.0016\n",
            "Optimal weights using Gradient Descent: [4766624.29995932  821964.61717253  300465.10632465  696251.22937515]\n",
            "Epoch 67, Loss: 895585055867.6135\n",
            "Optimal weights using Gradient Descent: [4766625.34943679  821964.72486927  300463.79024321  696252.43008486]\n",
            "Epoch 68, Loss: 895585055440.4901\n",
            "Optimal weights using Gradient Descent: [4766626.38841948  821964.831163    300462.48265361  696253.62351334]\n",
            "Epoch 69, Loss: 895585055019.536\n",
            "Optimal weights using Gradient Descent: [4766627.41701235  821964.93606893  300461.18350038  696254.80970429]\n",
            "Epoch 70, Loss: 895585054604.6576\n",
            "Optimal weights using Gradient Descent: [4766628.43531929  821965.03960213  300459.89272845  696255.98870116]\n",
            "Epoch 71, Loss: 895585054195.7627\n",
            "Optimal weights using Gradient Descent: [4766629.44344316  821965.14177751  300458.61028308  696257.16054713]\n",
            "Epoch 72, Loss: 895585053792.7604\n",
            "Optimal weights using Gradient Descent: [4766630.4414858   821965.24260984  300457.33610991  696258.32528511]\n",
            "Epoch 73, Loss: 895585053395.5613\n",
            "Optimal weights using Gradient Descent: [4766631.429548    821965.34211375  300456.07015494  696259.4829578 ]\n",
            "Epoch 74, Loss: 895585053004.0775\n",
            "Optimal weights using Gradient Descent: [4766632.40772959  821965.44030369  300454.81236451  696260.63360759]\n",
            "Epoch 75, Loss: 895585052618.223\n",
            "Optimal weights using Gradient Descent: [4766633.37612936  821965.537194    300453.56268534  696261.77727666]\n",
            "Epoch 76, Loss: 895585052237.9122\n",
            "Optimal weights using Gradient Descent: [4766634.33484513  821965.63279886  300452.32106449  696262.91400694]\n",
            "Epoch 77, Loss: 895585051863.0614\n",
            "Optimal weights using Gradient Descent: [4766635.28397374  821965.72713231  300451.08744937  696264.04384009]\n",
            "Epoch 78, Loss: 895585051493.5881\n",
            "Optimal weights using Gradient Descent: [4766636.22361107  821965.82020825  300449.86178773  696265.16681753]\n",
            "Epoch 79, Loss: 895585051129.4117\n",
            "Optimal weights using Gradient Descent: [4766637.15385202  821965.91204044  300448.6440277   696266.28298044]\n",
            "Epoch 80, Loss: 895585050770.4521\n",
            "Optimal weights using Gradient Descent: [4766638.07479056  821966.00264249  300447.43411771  696267.39236976]\n",
            "Epoch 81, Loss: 895585050416.6306\n",
            "Optimal weights using Gradient Descent: [4766638.98651972  821966.0920279   300446.23200655  696268.49502618]\n",
            "Epoch 82, Loss: 895585050067.87\n",
            "Optimal weights using Gradient Descent: [4766639.88913159  821966.18021002  300445.03764335  696269.59099014]\n",
            "Epoch 83, Loss: 895585049724.0945\n",
            "Optimal weights using Gradient Descent: [4766640.78271734  821966.26720205  300443.85097759  696270.68030187]\n",
            "Epoch 84, Loss: 895585049385.2288\n",
            "Optimal weights using Gradient Descent: [4766641.66736723  821966.35301707  300442.67195905  696271.76300133]\n",
            "Epoch 85, Loss: 895585049051.1993\n",
            "Optimal weights using Gradient Descent: [4766642.54317062  821966.43766804  300441.50053788  696272.83912826]\n",
            "Epoch 86, Loss: 895585048721.9341\n",
            "Optimal weights using Gradient Descent: [4766643.41021598  821966.52116778  300440.33666453  696273.90872215]\n",
            "Epoch 87, Loss: 895585048397.3611\n",
            "Optimal weights using Gradient Descent: [4766644.26859088  821966.60352897  300439.1802898   696274.97182228]\n",
            "Epoch 88, Loss: 895585048077.4106\n",
            "Optimal weights using Gradient Descent: [4766645.11838204  821966.68476418  300438.03136479  696276.02846767]\n",
            "Epoch 89, Loss: 895585047762.013\n",
            "Optimal weights using Gradient Descent: [4766645.95967528  821966.76488584  300436.88984094  696277.07869714]\n",
            "Epoch 90, Loss: 895585047451.1005\n",
            "Optimal weights using Gradient Descent: [4766646.79255559  821966.84390625  300435.75567002  696278.12254924]\n",
            "Epoch 91, Loss: 895585047144.6062\n",
            "Optimal weights using Gradient Descent: [4766647.6171071   821966.92183761  300434.62880411  696279.16006232]\n",
            "Epoch 92, Loss: 895585046842.4642\n",
            "Optimal weights using Gradient Descent: [4766648.43341309  821966.99869197  300433.50919559  696280.19127451]\n",
            "Epoch 93, Loss: 895585046544.6099\n",
            "Optimal weights using Gradient Descent: [4766649.24155603  821967.07448128  300432.39679718  696281.21622368]\n",
            "Epoch 94, Loss: 895585046250.9788\n",
            "Optimal weights using Gradient Descent: [4766650.04161753  821967.14921735  300431.2915619   696282.2349475 ]\n",
            "Epoch 95, Loss: 895585045961.5084\n",
            "Optimal weights using Gradient Descent: [4766650.83367842  821967.22291188  300430.19344308  696283.24748341]\n",
            "Epoch 96, Loss: 895585045676.137\n",
            "Optimal weights using Gradient Descent: [4766651.6178187   821967.29557646  300429.10239437  696284.25386864]\n",
            "Epoch 97, Loss: 895585045394.8035\n",
            "Optimal weights using Gradient Descent: [4766652.39411758  821967.36722254  300428.0183697   696285.25414017]\n",
            "Epoch 98, Loss: 895585045117.448\n",
            "Optimal weights using Gradient Descent: [4766653.16265347  821967.43786147  300426.94132333  696286.2483348 ]\n",
            "Epoch 99, Loss: 895585044844.0116\n",
            "Optimal weights using Gradient Descent: [4766653.923504    821967.50750449  300425.87120982  696287.23648906]\n",
            "Epoch 100, Loss: 895585044574.4362\n",
            "Optimal weights using Gradient Descent: [4766654.67674602  821967.57616271  300424.80798401  696288.21863932]\n",
            "Epoch 101, Loss: 895585044308.6644\n",
            "Optimal weights using Gradient Descent: [4766655.42245562  821967.64384713  300423.75160107  696289.1948217 ]\n",
            "Epoch 102, Loss: 895585044046.64\n",
            "Optimal weights using Gradient Descent: [4766656.16070813  821967.71056866  300422.70201643  696290.16507211]\n",
            "Epoch 103, Loss: 895585043788.308\n",
            "Optimal weights using Gradient Descent: [4766656.89157811  821967.77633807  300421.65918584  696291.12942624]\n",
            "Epoch 104, Loss: 895585043533.6135\n",
            "Optimal weights using Gradient Descent: [4766657.6151394   821967.84116604  300420.62306533  696292.0879196 ]\n",
            "Epoch 105, Loss: 895585043282.5033\n",
            "Optimal weights using Gradient Descent: [4766658.33146507  821967.90506312  300419.59361123  696293.04058745]\n",
            "Epoch 106, Loss: 895585043034.9237\n",
            "Optimal weights using Gradient Descent: [4766659.04062748  821967.96803979  300418.57078015  696293.98746485]\n",
            "Epoch 107, Loss: 895585042790.8232\n",
            "Optimal weights using Gradient Descent: [4766659.74269827  821968.03010639  300417.55452899  696294.92858668]\n",
            "Epoch 108, Loss: 895585042550.1505\n",
            "Optimal weights using Gradient Descent: [4766660.43774835  821968.09127315  300416.54481493  696295.86398758]\n",
            "Epoch 109, Loss: 895585042312.8553\n",
            "Optimal weights using Gradient Descent: [4766661.12584793  821968.15155024  300415.54159544  696296.793702  ]\n",
            "Epoch 110, Loss: 895585042078.888\n",
            "Optimal weights using Gradient Descent: [4766661.80706652  821968.21094767  300414.54482826  696297.71776417]\n",
            "Epoch 111, Loss: 895585041848.1992\n",
            "Optimal weights using Gradient Descent: [4766662.48147292  821968.2694754   300413.55447143  696298.63620814]\n",
            "Epoch 112, Loss: 895585041620.7416\n",
            "Optimal weights using Gradient Descent: [4766663.14913525  821968.32714324  300412.57048324  696299.54906774]\n",
            "Epoch 113, Loss: 895585041396.4672\n",
            "Optimal weights using Gradient Descent: [4766663.81012096  821968.38396094  300411.59282226  696300.45637661]\n",
            "Epoch 114, Loss: 895585041175.3295\n",
            "Optimal weights using Gradient Descent: [4766664.46449682  821968.43993812  300410.62144736  696301.35816818]\n",
            "Epoch 115, Loss: 895585040957.2826\n",
            "Optimal weights using Gradient Descent: [4766665.11232891  821968.49508433  300409.65631765  696302.25447568]\n",
            "Epoch 116, Loss: 895585040742.2815\n",
            "Optimal weights using Gradient Descent: [4766665.75368269  821968.54940899  300408.69739252  696303.14533215]\n",
            "Epoch 117, Loss: 895585040530.2815\n",
            "Optimal weights using Gradient Descent: [4766666.38862293  821968.60292146  300407.74463163  696304.03077044]\n",
            "Epoch 118, Loss: 895585040321.2385\n",
            "Optimal weights using Gradient Descent: [4766667.01721376  821968.65563098  300406.79799491  696304.91082319]\n",
            "Epoch 119, Loss: 895585040115.1099\n",
            "Optimal weights using Gradient Descent: [4766667.63951869  821968.70754669  300405.85744254  696305.78552285]\n",
            "Epoch 120, Loss: 895585039911.8529\n",
            "Optimal weights using Gradient Descent: [4766668.25560057  821968.75867766  300404.92293498  696306.65490169]\n",
            "Epoch 121, Loss: 895585039711.4258\n",
            "Optimal weights using Gradient Descent: [4766668.86552162  821968.80903285  300403.99443294  696307.51899176]\n",
            "Epoch 122, Loss: 895585039513.787\n",
            "Optimal weights using Gradient Descent: [4766669.46934347  821968.85862113  300403.07189739  696308.37782494]\n",
            "Epoch 123, Loss: 895585039318.8964\n",
            "Optimal weights using Gradient Descent: [4766670.0671271   821968.90745129  300402.15528956  696309.23143293]\n",
            "Epoch 124, Loss: 895585039126.7139\n",
            "Optimal weights using Gradient Descent: [4766670.6589329   821968.955532    300401.24457093  696310.07984722]\n",
            "Epoch 125, Loss: 895585038937.2\n",
            "Optimal weights using Gradient Descent: [4766671.24482063  821969.00287189  300400.33970324  696310.92309911]\n",
            "Epoch 126, Loss: 895585038750.3162\n",
            "Optimal weights using Gradient Descent: [4766671.82484949  821969.04947945  300399.44064847  696311.76121974]\n",
            "Epoch 127, Loss: 895585038566.0244\n",
            "Optimal weights using Gradient Descent: [4766672.39907806  821969.09536311  300398.54736888  696312.59424004]\n",
            "Epoch 128, Loss: 895585038384.2867\n",
            "Optimal weights using Gradient Descent: [4766672.96756434  821969.14053122  300397.65982694  696313.42219077]\n",
            "Epoch 129, Loss: 895585038205.0665\n",
            "Optimal weights using Gradient Descent: [4766673.53036576  821969.18499203  300396.77798539  696314.24510249]\n",
            "Epoch 130, Loss: 895585038028.3271\n",
            "Optimal weights using Gradient Descent: [4766674.08753917  821969.2287537   300395.90180721  696315.06300561]\n",
            "Epoch 131, Loss: 895585037854.0328\n",
            "Optimal weights using Gradient Descent: [4766674.63914084  821969.27182432  300395.03125563  696315.87593032]\n",
            "Epoch 132, Loss: 895585037682.1483\n",
            "Optimal weights using Gradient Descent: [4766675.1852265   821969.31421189  300394.16629411  696316.68390665]\n",
            "Epoch 133, Loss: 895585037512.6387\n",
            "Optimal weights using Gradient Descent: [4766675.7258513   821969.35592433  300393.30688636  696317.48696447]\n",
            "Epoch 134, Loss: 895585037345.4697\n",
            "Optimal weights using Gradient Descent: [4766676.26106985  821969.39696946  300392.45299632  696318.28513343]\n",
            "Epoch 135, Loss: 895585037180.6078\n",
            "Optimal weights using Gradient Descent: [4766676.79093621  821969.43735506  300391.60458818  696319.07844303]\n",
            "Epoch 136, Loss: 895585037018.0195\n",
            "Optimal weights using Gradient Descent: [4766677.31550391  821969.47708879  300390.76162636  696319.86692259]\n",
            "Epoch 137, Loss: 895585036857.6721\n",
            "Optimal weights using Gradient Descent: [4766677.83482594  821969.51617825  300389.92407551  696320.65060126]\n",
            "Epoch 138, Loss: 895585036699.5338\n",
            "Optimal weights using Gradient Descent: [4766678.34895474  821969.55463095  300389.09190051  696321.42950801]\n",
            "Epoch 139, Loss: 895585036543.5723\n",
            "Optimal weights using Gradient Descent: [4766678.85794226  821969.59245434  300388.26506648  696322.20367164]\n",
            "Epoch 140, Loss: 895585036389.7565\n",
            "Optimal weights using Gradient Descent: [4766679.3618399   821969.62965578  300387.44353877  696322.97312077]\n",
            "Epoch 141, Loss: 895585036238.056\n",
            "Optimal weights using Gradient Descent: [4766679.86069857  821969.66624255  300386.62728295  696323.73788385]\n",
            "Epoch 142, Loss: 895585036088.4396\n",
            "Optimal weights using Gradient Descent: [4766680.35456865  821969.70222186  300385.81626482  696324.49798919]\n",
            "Epoch 143, Loss: 895585035940.8785\n",
            "Optimal weights using Gradient Descent: [4766680.84350002  821969.73760085  300385.0104504   696325.25346489]\n",
            "Epoch 144, Loss: 895585035795.3427\n",
            "Optimal weights using Gradient Descent: [4766681.32754209  821969.77238657  300384.20980594  696326.0043389 ]\n",
            "Epoch 145, Loss: 895585035651.8031\n",
            "Optimal weights using Gradient Descent: [4766681.80674373  821969.80658603  300383.41429792  696326.75063901]\n",
            "Epoch 146, Loss: 895585035510.2313\n",
            "Optimal weights using Gradient Descent: [4766682.28115336  821969.84020612  300382.62389302  696327.49239284]\n",
            "Epoch 147, Loss: 895585035370.5996\n",
            "Optimal weights using Gradient Descent: [4766682.75081889  821969.8732537   300381.83855815  696328.22962784]\n",
            "Epoch 148, Loss: 895585035232.8798\n",
            "Optimal weights using Gradient Descent: [4766683.21578777  821969.90573554  300381.05826044  696328.96237129]\n",
            "Epoch 149, Loss: 895585035097.0448\n",
            "Optimal weights using Gradient Descent: [4766683.67610695  821969.93765833  300380.28296724  696329.69065033]\n",
            "Epoch 150, Loss: 895585034963.0676\n",
            "Optimal weights using Gradient Descent: [4766684.13182295  821969.96902871  300379.51264609  696330.41449192]\n",
            "Epoch 151, Loss: 895585034830.9219\n",
            "Optimal weights using Gradient Descent: [4766684.58298178  821969.99985325  300378.74726477  696331.13392286]\n",
            "Epoch 152, Loss: 895585034700.5815\n",
            "Optimal weights using Gradient Descent: [4766685.02962903  821970.03013843  300377.98679126  696331.8489698 ]\n",
            "Epoch 153, Loss: 895585034572.0211\n",
            "Optimal weights using Gradient Descent: [4766685.4718098   821970.05989068  300377.23119376  696332.55965922]\n",
            "Epoch 154, Loss: 895585034445.215\n",
            "Optimal weights using Gradient Descent: [4766685.90956877  821970.08911635  300376.48044067  696333.26601746]\n",
            "Epoch 155, Loss: 895585034320.1383\n",
            "Optimal weights using Gradient Descent: [4766686.34295014  821970.11782175  300375.73450059  696333.96807067]\n",
            "Epoch 156, Loss: 895585034196.7668\n",
            "Optimal weights using Gradient Descent: [4766686.77199771  821970.1460131   300374.99334234  696334.66584487]\n",
            "Epoch 157, Loss: 895585034075.0757\n",
            "Optimal weights using Gradient Descent: [4766687.1967548   821970.17369656  300374.25693493  696335.35936593]\n",
            "Epoch 158, Loss: 895585033955.0417\n",
            "Optimal weights using Gradient Descent: [4766687.61726431  821970.20087823  300373.52524761  696336.04865954]\n",
            "Epoch 159, Loss: 895585033836.6416\n",
            "Optimal weights using Gradient Descent: [4766688.03356873  821970.22756413  300372.79824978  696336.73375125]\n",
            "Epoch 160, Loss: 895585033719.8513\n",
            "Optimal weights using Gradient Descent: [4766688.44571011  821970.25376024  300372.07591107  696337.41466647]\n",
            "Epoch 161, Loss: 895585033604.6488\n",
            "Optimal weights using Gradient Descent: [4766688.85373007  821970.27947246  300371.35820131  696338.09143043]\n",
            "Epoch 162, Loss: 895585033491.0115\n",
            "Optimal weights using Gradient Descent: [4766689.25766984  821970.30470664  300370.64509053  696338.76406824]\n",
            "Epoch 163, Loss: 895585033378.9172\n",
            "Optimal weights using Gradient Descent: [4766689.6575702   821970.32946857  300369.93654893  696339.43260484]\n",
            "Epoch 164, Loss: 895585033268.3444\n",
            "Optimal weights using Gradient Descent: [4766690.05347156  821970.35376396  300369.23254695  696340.09706502]\n",
            "Epoch 165, Loss: 895585033159.2712\n",
            "Optimal weights using Gradient Descent: [4766690.44541391  821970.37759847  300368.53305518  696340.75747344]\n",
            "Epoch 166, Loss: 895585033051.6769\n",
            "Optimal weights using Gradient Descent: [4766690.83343684  821970.40097772  300367.83804443  696341.41385459]\n",
            "Epoch 167, Loss: 895585032945.5402\n",
            "Optimal weights using Gradient Descent: [4766691.21757953  821970.42390724  300367.14748569  696342.06623282]\n",
            "Epoch 168, Loss: 895585032840.8411\n",
            "Optimal weights using Gradient Descent: [4766691.5978808   821970.44639251  300366.46135015  696342.71463236]\n",
            "Epoch 169, Loss: 895585032737.5587\n",
            "Optimal weights using Gradient Descent: [4766691.97437906  821970.46843897  300365.77960918  696343.35907726]\n",
            "Epoch 170, Loss: 895585032635.6741\n",
            "Optimal weights using Gradient Descent: [4766692.34711233  821970.49005198  300365.10223434  696343.99959144]\n",
            "Epoch 171, Loss: 895585032535.1667\n",
            "Optimal weights using Gradient Descent: [4766692.71611827  821970.51123686  300364.42919738  696344.63619868]\n",
            "Epoch 172, Loss: 895585032436.018\n",
            "Optimal weights using Gradient Descent: [4766693.08143415  821970.53199886  300363.76047022  696345.26892261]\n",
            "Epoch 173, Loss: 895585032338.208\n",
            "Optimal weights using Gradient Descent: [4766693.44309688  821970.55234318  300363.09602499  696345.89778673]\n",
            "Epoch 174, Loss: 895585032241.7186\n",
            "Optimal weights using Gradient Descent: [4766693.80114297  821970.57227498  300362.43583399  696346.52281439]\n",
            "Epoch 175, Loss: 895585032146.5314\n",
            "Optimal weights using Gradient Descent: [4766694.15560861  821970.59179934  300361.77986969  696347.14402881]\n",
            "Epoch 176, Loss: 895585032052.6276\n",
            "Optimal weights using Gradient Descent: [4766694.50652958  821970.61092129  300361.12810476  696347.76145306]\n",
            "Epoch 177, Loss: 895585031959.9897\n",
            "Optimal weights using Gradient Descent: [4766694.85394135  821970.62964583  300360.48051203  696348.37511007]\n",
            "Epoch 178, Loss: 895585031868.5999\n",
            "Optimal weights using Gradient Descent: [4766695.197879    821970.64797788  300359.83706452  696348.98502266]\n",
            "Epoch 179, Loss: 895585031778.4406\n",
            "Optimal weights using Gradient Descent: [4766695.53837728  821970.66592232  300359.19773543  696349.59121347]\n",
            "Epoch 180, Loss: 895585031689.4949\n",
            "Optimal weights using Gradient Descent: [4766695.87547057  821970.68348399  300358.56249813  696350.19370503]\n",
            "Epoch 181, Loss: 895585031601.7455\n",
            "Optimal weights using Gradient Descent: [4766696.20919293  821970.70066764  300357.93132617  696350.79251975]\n",
            "Epoch 182, Loss: 895585031515.176\n",
            "Optimal weights using Gradient Descent: [4766696.53957806  821970.71747802  300357.30419325  696351.38767987]\n",
            "Epoch 183, Loss: 895585031429.7701\n",
            "Optimal weights using Gradient Descent: [4766696.86665935  821970.73391978  300356.68107328  696351.97920753]\n",
            "Epoch 184, Loss: 895585031345.5112\n",
            "Optimal weights using Gradient Descent: [4766697.19046982  821970.74999756  300356.06194031  696352.56712472]\n",
            "Epoch 185, Loss: 895585031262.3838\n",
            "Optimal weights using Gradient Descent: [4766697.51104218  821970.76571593  300355.44676857  696353.15145329]\n",
            "Epoch 186, Loss: 895585031180.3716\n",
            "Optimal weights using Gradient Descent: [4766697.82840883  821970.78107942  300354.83553246  696353.73221499]\n",
            "Epoch 187, Loss: 895585031099.4597\n",
            "Optimal weights using Gradient Descent: [4766698.1426018   821970.7960925   300354.22820655  696354.3094314 ]\n",
            "Epoch 188, Loss: 895585031019.6324\n",
            "Optimal weights using Gradient Descent: [4766698.45365285  821970.81075961  300353.62476557  696354.883124  ]\n",
            "Epoch 189, Loss: 895585030940.8748\n",
            "Optimal weights using Gradient Descent: [4766698.76159338  821970.82508512  300353.02518443  696355.45331414]\n",
            "Epoch 190, Loss: 895585030863.1719\n",
            "Optimal weights using Gradient Descent: [4766699.06645451  821970.83907337  300352.42943817  696356.02002302]\n",
            "Epoch 191, Loss: 895585030786.5093\n",
            "Optimal weights using Gradient Descent: [4766699.36826703  821970.85272864  300351.83750204  696356.58327174]\n",
            "Epoch 192, Loss: 895585030710.8726\n",
            "Optimal weights using Gradient Descent: [4766699.66706143  821970.86605519  300351.24935142  696357.14308125]\n",
            "Epoch 193, Loss: 895585030636.2473\n",
            "Optimal weights using Gradient Descent: [4766699.96286788  821970.87905721  300350.66496186  696357.6994724 ]\n",
            "Epoch 194, Loss: 895585030562.6195\n",
            "Optimal weights using Gradient Descent: [4766700.25571626  821970.89173884  300350.08430906  696358.25246589]\n",
            "Epoch 195, Loss: 895585030489.9758\n",
            "Optimal weights using Gradient Descent: [4766700.54563616  821970.9041042   300349.5073689   696358.80208231]\n",
            "Epoch 196, Loss: 895585030418.3022\n",
            "Optimal weights using Gradient Descent: [4766700.83265687  821970.91615735  300348.9341174   696359.34834212]\n",
            "Epoch 197, Loss: 895585030347.5856\n",
            "Optimal weights using Gradient Descent: [4766701.11680736  821970.92790231  300348.36453074  696359.89126567]\n",
            "Epoch 198, Loss: 895585030277.8123\n",
            "Optimal weights using Gradient Descent: [4766701.39811635  821970.93934305  300347.79858526  696360.43087317]\n",
            "Epoch 199, Loss: 895585030208.9696\n",
            "Optimal weights using Gradient Descent: [4766701.67661225  821970.95048351  300347.23625746  696360.96718472]\n",
            "Epoch 200, Loss: 895585030141.0449\n",
            "Optimal weights using Gradient Descent: [4766701.95232319  821970.96132758  300346.67752399  696361.5002203 ]\n",
            "Epoch 201, Loss: 895585030074.0251\n",
            "Optimal weights using Gradient Descent: [4766702.22527703  821970.97187909  300346.12236163  696362.02999976]\n",
            "Epoch 202, Loss: 895585030007.8982\n",
            "Optimal weights using Gradient Descent: [4766702.49550132  821970.98214187  300345.57074735  696362.55654285]\n",
            "Epoch 203, Loss: 895585029942.6514\n",
            "Optimal weights using Gradient Descent: [4766702.76302337  821970.99211968  300345.02265824  696363.07986918]\n",
            "Epoch 204, Loss: 895585029878.273\n",
            "Optimal weights using Gradient Descent: [4766703.0278702   821971.00181623  300344.47807155  696363.59999826]\n",
            "Epoch 205, Loss: 895585029814.7512\n",
            "Optimal weights using Gradient Descent: [4766703.29006856  821971.01123522  300343.93696469  696364.11694947]\n",
            "Epoch 206, Loss: 895585029752.0735\n",
            "Optimal weights using Gradient Descent: [4766703.54964494  821971.02038029  300343.39931521  696364.63074207]\n",
            "Epoch 207, Loss: 895585029690.2289\n",
            "Optimal weights using Gradient Descent: [4766703.80662556  821971.02925504  300342.86510079  696365.14139524]\n",
            "Epoch 208, Loss: 895585029629.206\n",
            "Optimal weights using Gradient Descent: [4766704.06103637  821971.03786303  300342.33429927  696365.64892799]\n",
            "Epoch 209, Loss: 895585029568.9934\n",
            "Optimal weights using Gradient Descent: [4766704.31290307  821971.0462078   300341.80688865  696366.15335926]\n",
            "Epoch 210, Loss: 895585029509.5797\n",
            "Optimal weights using Gradient Descent: [4766704.5622511   821971.05429283  300341.28284705  696366.65470786]\n",
            "Epoch 211, Loss: 895585029450.9545\n",
            "Optimal weights using Gradient Descent: [4766704.80910565  821971.06212157  300340.76215274  696367.15299249]\n",
            "Epoch 212, Loss: 895585029393.1066\n",
            "Optimal weights using Gradient Descent: [4766705.05349166  821971.06969744  300340.24478413  696367.64823173]\n",
            "Epoch 213, Loss: 895585029336.0256\n",
            "Optimal weights using Gradient Descent: [4766705.29543381  821971.0770238   300339.73071978  696368.14044405]\n",
            "Epoch 214, Loss: 895585029279.7008\n",
            "Optimal weights using Gradient Descent: [4766705.53495654  821971.084104    300339.21993839  696368.62964783]\n",
            "Epoch 215, Loss: 895585029224.1222\n",
            "Optimal weights using Gradient Descent: [4766705.77208403  821971.09094133  300338.71241878  696369.1158613 ]\n",
            "Epoch 216, Loss: 895585029169.2793\n",
            "Optimal weights using Gradient Descent: [4766706.00684026  821971.09753907  300338.20813992  696369.59910262]\n",
            "Epoch 217, Loss: 895585029115.1622\n",
            "Optimal weights using Gradient Descent: [4766706.23924892  821971.10390044  300337.70708094  696370.07938983]\n",
            "Epoch 218, Loss: 895585029061.7609\n",
            "Optimal weights using Gradient Descent: [4766706.46933349  821971.11002863  300337.20922106  696370.55674083]\n",
            "Epoch 219, Loss: 895585029009.0657\n",
            "Optimal weights using Gradient Descent: [4766706.69711722  821971.11592681  300336.71453967  696371.03117347]\n",
            "Epoch 220, Loss: 895585028957.0669\n",
            "Optimal weights using Gradient Descent: [4766706.92262312  821971.12159809  300336.22301628  696371.50270544]\n",
            "Epoch 221, Loss: 895585028905.7554\n",
            "Optimal weights using Gradient Descent: [4766707.14587395  821971.12704558  300335.73463055  696371.97135436]\n",
            "Epoch 222, Loss: 895585028855.1212\n",
            "Optimal weights using Gradient Descent: [4766707.36689227  821971.13227231  300335.24936226  696372.43713771]\n",
            "Epoch 223, Loss: 895585028805.1556\n",
            "Optimal weights using Gradient Descent: [4766707.58570042  821971.13728132  300334.76719131  696372.9000729 ]\n",
            "Epoch 224, Loss: 895585028755.8494\n",
            "Optimal weights using Gradient Descent: [4766707.80232047  821971.1420756   300334.28809775  696373.36017722]\n",
            "Epoch 225, Loss: 895585028707.1935\n",
            "Optimal weights using Gradient Descent: [4766708.01677433  821971.14665809  300333.81206175  696373.81746786]\n",
            "Epoch 226, Loss: 895585028659.1791\n",
            "Optimal weights using Gradient Descent: [4766708.22908365  821971.15103172  300333.33906363  696374.27196188]\n",
            "Epoch 227, Loss: 895585028611.7975\n",
            "Optimal weights using Gradient Descent: [4766708.43926988  821971.15519939  300332.86908381  696374.72367628]\n",
            "Epoch 228, Loss: 895585028565.0404\n",
            "Optimal weights using Gradient Descent: [4766708.64735425  821971.15916394  300332.40210284  696375.17262794]\n",
            "Epoch 229, Loss: 895585028518.8989\n",
            "Optimal weights using Gradient Descent: [4766708.85335777  821971.16292822  300331.93810143  696375.61883363]\n",
            "Epoch 230, Loss: 895585028473.3649\n",
            "Optimal weights using Gradient Descent: [4766709.05730125  821971.166495    300331.47706037  696376.06231002]\n",
            "Epoch 231, Loss: 895585028428.43\n",
            "Optimal weights using Gradient Descent: [4766709.25920531  821971.16986706  300331.0189606   696376.50307369]\n",
            "Epoch 232, Loss: 895585028384.0863\n",
            "Optimal weights using Gradient Descent: [4766709.45909032  821971.17304713  300330.56378319  696376.94114112]\n",
            "Epoch 233, Loss: 895585028340.3257\n",
            "Optimal weights using Gradient Descent: [4766709.65697648  821971.17603792  300330.11150933  696377.37652869]\n",
            "Epoch 234, Loss: 895585028297.1404\n",
            "Optimal weights using Gradient Descent: [4766709.85288378  821971.17884209  300329.66212031  696377.80925267]\n",
            "Epoch 235, Loss: 895585028254.5226\n",
            "Optimal weights using Gradient Descent: [4766710.046832    821971.18146228  300329.21559756  696378.23932924]\n",
            "Epoch 236, Loss: 895585028212.4645\n",
            "Optimal weights using Gradient Descent: [4766710.23884075  821971.18390112  300328.77192265  696378.66677448]\n",
            "Epoch 237, Loss: 895585028170.9584\n",
            "Optimal weights using Gradient Descent: [4766710.42892941  821971.18616119  300328.33107723  696379.09160439]\n",
            "Epoch 238, Loss: 895585028129.9974\n",
            "Optimal weights using Gradient Descent: [4766710.61711718  821971.18824503  300327.8930431   696379.51383486]\n",
            "Epoch 239, Loss: 895585028089.5739\n",
            "Optimal weights using Gradient Descent: [4766710.80342307  821971.19015519  300327.45780217  696379.93348167]\n",
            "Epoch 240, Loss: 895585028049.6802\n",
            "Optimal weights using Gradient Descent: [4766710.9878659   821971.19189414  300327.02533645  696380.35056053]\n",
            "Epoch 241, Loss: 895585028010.3097\n",
            "Optimal weights using Gradient Descent: [4766711.17046431  821971.19346438  300326.5956281   696380.76508704]\n",
            "Epoch 242, Loss: 895585027971.4552\n",
            "Optimal weights using Gradient Descent: [4766711.35123673  821971.19486833  300326.16865937  696381.17707671]\n",
            "Epoch 243, Loss: 895585027933.1097\n",
            "Optimal weights using Gradient Descent: [4766711.53020143  821971.19610842  300325.74441264  696381.58654497]\n",
            "Epoch 244, Loss: 895585027895.2664\n",
            "Optimal weights using Gradient Descent: [4766711.70737648  821971.19718703  300325.3228704   696381.99350714]\n",
            "Epoch 245, Loss: 895585027857.9188\n",
            "Optimal weights using Gradient Descent: [4766711.88277978  821971.19810652  300324.90401524  696382.39797844]\n",
            "Epoch 246, Loss: 895585027821.0597\n",
            "Optimal weights using Gradient Descent: [4766712.05642904  821971.19886923  300324.48782989  696382.79997403]\n",
            "Epoch 247, Loss: 895585027784.6829\n",
            "Optimal weights using Gradient Descent: [4766712.22834182  821971.19947746  300324.07429718  696383.19950895]\n",
            "Epoch 248, Loss: 895585027748.7819\n",
            "Optimal weights using Gradient Descent: [4766712.39853546  821971.19993349  300323.66340005  696383.59659816]\n",
            "Epoch 249, Loss: 895585027713.3502\n",
            "Optimal weights using Gradient Descent: [4766712.56702717  821971.20023959  300323.25512155  696383.99125653]\n",
            "Epoch 250, Loss: 895585027678.3816\n",
            "Optimal weights using Gradient Descent: [4766712.73383396  821971.20039798  300322.84944483  696384.38349885]\n",
            "Epoch 251, Loss: 895585027643.8698\n",
            "Optimal weights using Gradient Descent: [4766712.89897269  821971.20041086  300322.44635318  696384.77333979]\n",
            "Epoch 252, Loss: 895585027609.8087\n",
            "Optimal weights using Gradient Descent: [4766713.06246003  821971.20028042  300322.04582997  696385.16079397]\n",
            "Epoch 253, Loss: 895585027576.1924\n",
            "Optimal weights using Gradient Descent: [4766713.22431249  821971.20000881  300321.64785869  696385.5458759 ]\n",
            "Epoch 254, Loss: 895585027543.0149\n",
            "Optimal weights using Gradient Descent: [4766713.38454643  821971.19959816  300321.25242294  696385.92860001]\n",
            "Epoch 255, Loss: 895585027510.2701\n",
            "Optimal weights using Gradient Descent: [4766713.54317803  821971.19905058  300320.85950641  696386.30898063]\n",
            "Epoch 256, Loss: 895585027477.9525\n",
            "Optimal weights using Gradient Descent: [4766713.70022331  821971.19836814  300320.46909293  696386.68703202]\n",
            "Epoch 257, Loss: 895585027446.056\n",
            "Optimal weights using Gradient Descent: [4766713.85569814  821971.19755292  300320.08116639  696387.06276835]\n",
            "Epoch 258, Loss: 895585027414.5756\n",
            "Optimal weights using Gradient Descent: [4766714.00961823  821971.19660693  300319.69571082  696387.43620371]\n",
            "Epoch 259, Loss: 895585027383.5052\n",
            "Optimal weights using Gradient Descent: [4766714.16199911  821971.1955322   300319.31271035  696387.80735208]\n",
            "Epoch 260, Loss: 895585027352.8397\n",
            "Optimal weights using Gradient Descent: [4766714.31285618  821971.19433071  300318.93214919  696388.17622738]\n",
            "Epoch 261, Loss: 895585027322.5731\n",
            "Optimal weights using Gradient Descent: [4766714.46220468  821971.19300442  300318.55401168  696388.54284345]\n",
            "Epoch 262, Loss: 895585027292.7008\n",
            "Optimal weights using Gradient Descent: [4766714.6100597   821971.19155529  300318.17828224  696388.90721402]\n",
            "Epoch 263, Loss: 895585027263.2173\n",
            "Optimal weights using Gradient Descent: [4766714.75643617  821971.18998522  300317.80494541  696389.26935277]\n",
            "Epoch 264, Loss: 895585027234.1171\n",
            "Optimal weights using Gradient Descent: [4766714.90134887  821971.18829612  300317.43398581  696389.62927326]\n",
            "Epoch 265, Loss: 895585027205.3955\n",
            "Optimal weights using Gradient Descent: [4766715.04481245  821971.18648987  300317.06538818  696389.98698901]\n",
            "Epoch 266, Loss: 895585027177.0471\n",
            "Optimal weights using Gradient Descent: [4766715.18684139  821971.18456831  300316.69913735  696390.34251343]\n",
            "Epoch 267, Loss: 895585027149.0674\n",
            "Optimal weights using Gradient Descent: [4766715.32745004  821971.18253328  300316.33521824  696390.69585986]\n",
            "Epoch 268, Loss: 895585027121.4509\n",
            "Optimal weights using Gradient Descent: [4766715.4666526   821971.1803866   300315.97361589  696391.04704155]\n",
            "Epoch 269, Loss: 895585027094.1935\n",
            "Optimal weights using Gradient Descent: [4766715.60446314  821971.17813005  300315.61431542  696391.39607167]\n",
            "Epoch 270, Loss: 895585027067.2898\n",
            "Optimal weights using Gradient Descent: [4766715.74089557  821971.17576541  300315.25730204  696391.74296334]\n",
            "Epoch 271, Loss: 895585027040.7352\n",
            "Optimal weights using Gradient Descent: [4766715.87596368  821971.17329442  300314.90256108  696392.08772956]\n",
            "Epoch 272, Loss: 895585027014.5254\n",
            "Optimal weights using Gradient Descent: [4766716.00968111  821971.17071881  300314.55007795  696392.43038327]\n",
            "Epoch 273, Loss: 895585026988.6555\n",
            "Optimal weights using Gradient Descent: [4766716.14206136  821971.16804029  300314.19983815  696392.77093734]\n",
            "Epoch 274, Loss: 895585026963.1212\n",
            "Optimal weights using Gradient Descent: [4766716.27311781  821971.16526055  300313.85182728  696393.10940455]\n",
            "Epoch 275, Loss: 895585026937.9178\n",
            "Optimal weights using Gradient Descent: [4766716.4028637   821971.16238126  300313.50603104  696393.4457976 ]\n",
            "Epoch 276, Loss: 895585026913.0414\n",
            "Optimal weights using Gradient Descent: [4766716.53131212  821971.15940407  300313.16243521  696393.78012913]\n",
            "Epoch 277, Loss: 895585026888.487\n",
            "Optimal weights using Gradient Descent: [4766716.65847607  821971.15633061  300312.82102568  696394.11241169]\n",
            "Epoch 278, Loss: 895585026864.2507\n",
            "Optimal weights using Gradient Descent: [4766716.78436837  821971.15316248  300312.48178841  696394.44265776]\n",
            "Epoch 279, Loss: 895585026840.3285\n",
            "Optimal weights using Gradient Descent: [4766716.90900175  821971.14990129  300312.14470947  696394.77087974]\n",
            "Epoch 280, Loss: 895585026816.7158\n",
            "Optimal weights using Gradient Descent: [4766717.0323888   821971.14654861  300311.80977501  696395.09708996]\n",
            "Epoch 281, Loss: 895585026793.4086\n",
            "Optimal weights using Gradient Descent: [4766717.15454197  821971.14310598  300311.47697127  696395.42130067]\n",
            "Epoch 282, Loss: 895585026770.4031\n",
            "Optimal weights using Gradient Descent: [4766717.27547362  821971.13957495  300311.14628458  696395.74352406]\n",
            "Epoch 283, Loss: 895585026747.695\n",
            "Optimal weights using Gradient Descent: [4766717.39519595  821971.13595703  300310.81770136  696396.06377222]\n",
            "Epoch 284, Loss: 895585026725.2808\n",
            "Optimal weights using Gradient Descent: [4766717.51372105  821971.13225372  300310.49120812  696396.38205719]\n",
            "Epoch 285, Loss: 895585026703.1564\n",
            "Optimal weights using Gradient Descent: [4766717.63106091  821971.12846652  300310.16679146  696396.69839094]\n",
            "Epoch 286, Loss: 895585026681.3177\n",
            "Optimal weights using Gradient Descent: [4766717.74722736  821971.12459687  300309.84443805  696397.01278535]\n",
            "Epoch 287, Loss: 895585026659.7612\n",
            "Optimal weights using Gradient Descent: [4766717.86223215  821971.12064624  300309.52413467  696397.32525223]\n",
            "Epoch 288, Loss: 895585026638.4832\n",
            "Optimal weights using Gradient Descent: [4766717.97608689  821971.11661604  300309.20586817  696397.63580333]\n",
            "Epoch 289, Loss: 895585026617.48\n",
            "Optimal weights using Gradient Descent: [4766718.08880309  821971.11250769  300308.8896255   696397.94445033]\n",
            "Epoch 290, Loss: 895585026596.7478\n",
            "Optimal weights using Gradient Descent: [4766718.20039212  821971.10832259  300308.57539366  696398.25120483]\n",
            "Epoch 291, Loss: 895585026576.2833\n",
            "Optimal weights using Gradient Descent: [4766718.31086527  821971.10406212  300308.26315978  696398.55607836]\n",
            "Epoch 292, Loss: 895585026556.0831\n",
            "Optimal weights using Gradient Descent: [4766718.42023368  821971.09972764  300307.95291105  696398.85908238]\n",
            "Epoch 293, Loss: 895585026536.143\n",
            "Optimal weights using Gradient Descent: [4766718.5285084   821971.09532049  300307.64463473  696399.1602283 ]\n",
            "Epoch 294, Loss: 895585026516.4603\n",
            "Optimal weights using Gradient Descent: [4766718.63570038  821971.09084201  300307.3383182   696399.45952743]\n",
            "Epoch 295, Loss: 895585026497.0314\n",
            "Optimal weights using Gradient Descent: [4766718.74182044  821971.08629351  300307.03394887  696399.75699103]\n",
            "Epoch 296, Loss: 895585026477.8527\n",
            "Optimal weights using Gradient Descent: [4766718.8468793   821971.0816763   300306.73151429  696400.0526303 ]\n",
            "Epoch 297, Loss: 895585026458.9214\n",
            "Optimal weights using Gradient Descent: [4766718.95088758  821971.07699164  300306.43100205  696400.34645635]\n",
            "Epoch 298, Loss: 895585026440.2338\n",
            "Optimal weights using Gradient Descent: [4766719.05385576  821971.07224081  300306.13239983  696400.63848024]\n",
            "Epoch 299, Loss: 895585026421.7869\n",
            "Optimal weights using Gradient Descent: [4766719.15579427  821971.06742507  300305.83569539  696400.92871295]\n",
            "Epoch 300, Loss: 895585026403.5778\n",
            "Optimal weights using Gradient Descent: [4766719.25671339  821971.06254564  300305.54087657  696401.21716541]\n",
            "Epoch 301, Loss: 895585026385.6027\n",
            "Optimal weights using Gradient Descent: [4766719.35662332  821971.05760375  300305.2479313   696401.50384848]\n",
            "Epoch 302, Loss: 895585026367.859\n",
            "Optimal weights using Gradient Descent: [4766719.45553415  821971.05260061  300304.95684756  696401.78877293]\n",
            "Epoch 303, Loss: 895585026350.3438\n",
            "Optimal weights using Gradient Descent: [4766719.55345588  821971.0475374   300304.66761345  696402.0719495 ]\n",
            "Epoch 304, Loss: 895585026333.0537\n",
            "Optimal weights using Gradient Descent: [4766719.65039838  821971.04241531  300304.3802171   696402.35338885]\n",
            "Epoch 305, Loss: 895585026315.9861\n",
            "Optimal weights using Gradient Descent: [4766719.74637146  821971.0372355   300304.09464675  696402.63310156]\n",
            "Epoch 306, Loss: 895585026299.138\n",
            "Optimal weights using Gradient Descent: [4766719.84138481  821971.03199912  300303.8108907   696402.91109818]\n",
            "Epoch 307, Loss: 895585026282.5061\n",
            "Optimal weights using Gradient Descent: [4766719.93544803  821971.02670729  300303.52893734  696403.18738916]\n",
            "Epoch 308, Loss: 895585026266.0883\n",
            "Optimal weights using Gradient Descent: [4766720.02857061  821971.02136115  300303.24877513  696403.46198492]\n",
            "Epoch 309, Loss: 895585026249.8811\n",
            "Optimal weights using Gradient Descent: [4766720.12076197  821971.0159618   300302.97039258  696403.73489579]\n",
            "Epoch 310, Loss: 895585026233.8823\n",
            "Optimal weights using Gradient Descent: [4766720.21203141  821971.01051033  300302.69377831  696404.00613205]\n",
            "Epoch 311, Loss: 895585026218.089\n",
            "Optimal weights using Gradient Descent: [4766720.30238816  821971.00500782  300302.418921    696404.27570391]\n",
            "Epoch 312, Loss: 895585026202.4982\n",
            "Optimal weights using Gradient Descent: [4766720.39184135  821970.99945535  300302.1458094   696404.54362154]\n",
            "Epoch 313, Loss: 895585026187.1078\n",
            "Optimal weights using Gradient Descent: [4766720.4804      821970.99385395  300301.87443233  696404.80989503]\n",
            "Epoch 314, Loss: 895585026171.9147\n",
            "Optimal weights using Gradient Descent: [4766720.56807306  821970.98820467  300301.6047787   696405.0745344 ]\n",
            "Epoch 315, Loss: 895585026156.9165\n",
            "Optimal weights using Gradient Descent: [4766720.6548694   821970.98250854  300301.33683747  696405.33754962]\n",
            "Epoch 316, Loss: 895585026142.1107\n",
            "Optimal weights using Gradient Descent: [4766720.74079777  821970.97676658  300301.07059767  696405.59895062]\n",
            "Epoch 317, Loss: 895585026127.4949\n",
            "Optimal weights using Gradient Descent: [4766720.82586685  821970.97097977  300300.80604843  696405.85874724]\n",
            "Epoch 318, Loss: 895585026113.0663\n",
            "Optimal weights using Gradient Descent: [4766720.91008525  821970.96514912  300300.54317893  696406.11694926]\n",
            "Epoch 319, Loss: 895585026098.8228\n",
            "Optimal weights using Gradient Descent: [4766720.99346146  821970.9592756   300300.28197841  696406.37356643]\n",
            "Epoch 320, Loss: 895585026084.7616\n",
            "Optimal weights using Gradient Descent: [4766721.07600391  821970.95336017  300300.02243619  696406.62860842]\n",
            "Epoch 321, Loss: 895585026070.8806\n",
            "Optimal weights using Gradient Descent: [4766721.15772093  821970.94740379  300299.76454168  696406.88208483]\n",
            "Epoch 322, Loss: 895585026057.1776\n",
            "Optimal weights using Gradient Descent: [4766721.23862079  821970.94140739  300299.50828432  696407.13400523]\n",
            "Epoch 323, Loss: 895585026043.6497\n",
            "Optimal weights using Gradient Descent: [4766721.31871165  821970.93537191  300299.25365364  696407.38437912]\n",
            "Epoch 324, Loss: 895585026030.2953\n",
            "Optimal weights using Gradient Descent: [4766721.39800159  821970.92929825  300299.00063924  696407.63321593]\n",
            "Epoch 325, Loss: 895585026017.1118\n",
            "Optimal weights using Gradient Descent: [4766721.47649864  821970.92318733  300298.74923078  696407.88052505]\n",
            "Epoch 326, Loss: 895585026004.097\n",
            "Optimal weights using Gradient Descent: [4766721.55421072  821970.91704004  300298.49941799  696408.12631581]\n",
            "Epoch 327, Loss: 895585025991.2488\n",
            "Optimal weights using Gradient Descent: [4766721.63114568  821970.91085726  300298.25119066  696408.37059747]\n",
            "Epoch 328, Loss: 895585025978.5647\n",
            "Optimal weights using Gradient Descent: [4766721.70731128  821970.90463986  300298.00453865  696408.61337926]\n",
            "Epoch 329, Loss: 895585025966.0432\n",
            "Optimal weights using Gradient Descent: [4766721.78271523  821970.89838869  300297.7594519   696408.85467032]\n",
            "Epoch 330, Loss: 895585025953.6816\n",
            "Optimal weights using Gradient Descent: [4766721.85736515  821970.89210462  300297.51592038  696409.09447977]\n",
            "Epoch 331, Loss: 895585025941.4778\n",
            "Optimal weights using Gradient Descent: [4766721.93126856  821970.88578847  300297.27393417  696409.33281665]\n",
            "Epoch 332, Loss: 895585025929.4303\n",
            "Optimal weights using Gradient Descent: [4766722.00443294  821970.87944107  300297.03348337  696409.56968995]\n",
            "Epoch 333, Loss: 895585025917.5365\n",
            "Optimal weights using Gradient Descent: [4766722.07686567  821970.87306324  300296.79455817  696409.80510861]\n",
            "Epoch 334, Loss: 895585025905.795\n",
            "Optimal weights using Gradient Descent: [4766722.14857408  821970.86665579  300296.55714882  696410.03908152]\n",
            "Epoch 335, Loss: 895585025894.2032\n",
            "Optimal weights using Gradient Descent: [4766722.2195654   821970.8602195   300296.32124563  696410.2716175 ]\n",
            "Epoch 336, Loss: 895585025882.7595\n",
            "Optimal weights using Gradient Descent: [4766722.28984681  821970.85375517  300296.08683897  696410.50272534]\n",
            "Epoch 337, Loss: 895585025871.462\n",
            "Optimal weights using Gradient Descent: [4766722.35942541  821970.84726356  300295.85391928  696410.73241375]\n",
            "Epoch 338, Loss: 895585025860.3086\n",
            "Optimal weights using Gradient Descent: [4766722.42830822  821970.84074545  300295.62247706  696410.96069142]\n",
            "Epoch 339, Loss: 895585025849.2977\n",
            "Optimal weights using Gradient Descent: [4766722.4965022   821970.83420158  300295.39250286  696411.18756695]\n",
            "Epoch 340, Loss: 895585025838.4271\n",
            "Optimal weights using Gradient Descent: [4766722.56401424  821970.8276327   300295.1639873   696411.41304891]\n",
            "Epoch 341, Loss: 895585025827.6953\n",
            "Optimal weights using Gradient Descent: [4766722.63085117  821970.82103955  300294.93692106  696411.63714582]\n",
            "Epoch 342, Loss: 895585025817.1003\n",
            "Optimal weights using Gradient Descent: [4766722.69701972  821970.81442285  300294.71129488  696411.85986614]\n",
            "Epoch 343, Loss: 895585025806.6406\n",
            "Optimal weights using Gradient Descent: [4766722.76252659  821970.80778332  300294.48709957  696412.08121828]\n",
            "Epoch 344, Loss: 895585025796.3142\n",
            "Optimal weights using Gradient Descent: [4766722.82737838  821970.80112165  300294.26432597  696412.3012106 ]\n",
            "Epoch 345, Loss: 895585025786.1195\n",
            "Optimal weights using Gradient Descent: [4766722.89158166  821970.79443856  300294.04296501  696412.51985141]\n",
            "Epoch 346, Loss: 895585025776.0547\n",
            "Optimal weights using Gradient Descent: [4766722.95514291  821970.78773472  300293.82300767  696412.73714898]\n",
            "Epoch 347, Loss: 895585025766.1179\n",
            "Optimal weights using Gradient Descent: [4766723.01806855  821970.78101082  300293.60444497  696412.9531115 ]\n",
            "Epoch 348, Loss: 895585025756.3081\n",
            "Optimal weights using Gradient Descent: [4766723.08036492  821970.77426752  300293.38726802  696413.16774714]\n",
            "Epoch 349, Loss: 895585025746.6229\n",
            "Optimal weights using Gradient Descent: [4766723.14203834  821970.76750548  300293.17146796  696413.381064  ]\n",
            "Epoch 350, Loss: 895585025737.0613\n",
            "Optimal weights using Gradient Descent: [4766723.20309502  821970.76072536  300292.95703599  696413.59307016]\n",
            "Epoch 351, Loss: 895585025727.6213\n",
            "Optimal weights using Gradient Descent: [4766723.26354114  821970.75392779  300292.74396339  696413.80377361]\n",
            "Epoch 352, Loss: 895585025718.3016\n",
            "Optimal weights using Gradient Descent: [4766723.32338279  821970.74711342  300292.53224147  696414.01318233]\n",
            "Epoch 353, Loss: 895585025709.1005\n",
            "Optimal weights using Gradient Descent: [4766723.38262603  821970.74028286  300292.32186161  696414.22130423]\n",
            "Epoch 354, Loss: 895585025700.0165\n",
            "Optimal weights using Gradient Descent: [4766723.44127683  821970.73343673  300292.11281523  696414.42814717]\n",
            "Epoch 355, Loss: 895585025691.0482\n",
            "Optimal weights using Gradient Descent: [4766723.49934112  821970.72657565  300291.90509384  696414.63371898]\n",
            "Epoch 356, Loss: 895585025682.1938\n",
            "Optimal weights using Gradient Descent: [4766723.55682478  821970.7197002   300291.69868896  696414.83802742]\n",
            "Epoch 357, Loss: 895585025673.4521\n",
            "Optimal weights using Gradient Descent: [4766723.61373359  821970.71281099  300291.49359219  696415.04108022]\n",
            "Epoch 358, Loss: 895585025664.8218\n",
            "Optimal weights using Gradient Descent: [4766723.67007332  821970.7059086   300291.28979519  696415.24288506]\n",
            "Epoch 359, Loss: 895585025656.3008\n",
            "Optimal weights using Gradient Descent: [4766723.72584965  821970.6989936   300291.08728966  696415.44344957]\n",
            "Epoch 360, Loss: 895585025647.8884\n",
            "Optimal weights using Gradient Descent: [4766723.78106822  821970.69206656  300290.88606735  696415.64278133]\n",
            "Epoch 361, Loss: 895585025639.5831\n",
            "Optimal weights using Gradient Descent: [4766723.8357346   821970.68512805  300290.68612008  696415.84088788]\n",
            "Epoch 362, Loss: 895585025631.383\n",
            "Optimal weights using Gradient Descent: [4766723.88985432  821970.6781786   300290.48743971  696416.03777672]\n",
            "Epoch 363, Loss: 895585025623.2872\n",
            "Optimal weights using Gradient Descent: [4766723.94343284  821970.67121877  300290.29001816  696416.23345528]\n",
            "Epoch 364, Loss: 895585025615.2943\n",
            "Optimal weights using Gradient Descent: [4766723.99647558  821970.6642491   300290.09384739  696416.42793099]\n",
            "Epoch 365, Loss: 895585025607.4031\n",
            "Optimal weights using Gradient Descent: [4766724.04898789  821970.65727011  300289.89891943  696416.62121118]\n",
            "Epoch 366, Loss: 895585025599.6118\n",
            "Optimal weights using Gradient Descent: [4766724.10097507  821970.65028233  300289.70522634  696416.81330318]\n",
            "Epoch 367, Loss: 895585025591.9196\n",
            "Optimal weights using Gradient Descent: [4766724.15244239  821970.64328627  300289.51276024  696417.00421424]\n",
            "Epoch 368, Loss: 895585025584.3252\n",
            "Optimal weights using Gradient Descent: [4766724.20339503  821970.63628243  300289.32151332  696417.19395161]\n",
            "Epoch 369, Loss: 895585025576.8269\n",
            "Optimal weights using Gradient Descent: [4766724.25383814  821970.62927133  300289.13147779  696417.38252245]\n",
            "Epoch 370, Loss: 895585025569.424\n",
            "Optimal weights using Gradient Descent: [4766724.30377682  821970.62225344  300288.94264592  696417.5699339 ]\n",
            "Epoch 371, Loss: 895585025562.115\n",
            "Optimal weights using Gradient Descent: [4766724.35321612  821970.61522927  300288.75501005  696417.75619306]\n",
            "Epoch 372, Loss: 895585025554.8987\n",
            "Optimal weights using Gradient Descent: [4766724.40216102  821970.60819928  300288.56856253  696417.94130697]\n",
            "Epoch 373, Loss: 895585025547.7739\n",
            "Optimal weights using Gradient Descent: [4766724.45061648  821970.60116394  300288.3832958   696418.12528264]\n",
            "Epoch 374, Loss: 895585025540.7396\n",
            "Optimal weights using Gradient Descent: [4766724.49858737  821970.59412374  300288.19920233  696418.30812704]\n",
            "Epoch 375, Loss: 895585025533.7944\n",
            "Optimal weights using Gradient Descent: [4766724.54607856  821970.58707912  300288.01627462  696418.48984709]\n",
            "Epoch 376, Loss: 895585025526.9373\n",
            "Optimal weights using Gradient Descent: [4766724.59309484  821970.58003053  300287.83450526  696418.67044967]\n",
            "Epoch 377, Loss: 895585025520.167\n",
            "Optimal weights using Gradient Descent: [4766724.63964096  821970.57297842  300287.65388685  696418.84994162]\n",
            "Epoch 378, Loss: 895585025513.4828\n",
            "Optimal weights using Gradient Descent: [4766724.68572161  821970.56592324  300287.47441205  696419.02832973]\n",
            "Epoch 379, Loss: 895585025506.883\n",
            "Optimal weights using Gradient Descent: [4766724.73134146  821970.55886541  300287.29607359  696419.20562077]\n",
            "Epoch 380, Loss: 895585025500.3671\n",
            "Optimal weights using Gradient Descent: [4766724.77650511  821970.55180536  300287.1188642   696419.38182143]\n",
            "Epoch 381, Loss: 895585025493.9337\n",
            "Optimal weights using Gradient Descent: [4766724.82121713  821970.54474351  300286.9427767   696419.55693841]\n",
            "Epoch 382, Loss: 895585025487.5817\n",
            "Optimal weights using Gradient Descent: [4766724.86548202  821970.53768027  300286.76780393  696419.73097833]\n",
            "Epoch 383, Loss: 895585025481.3103\n",
            "Optimal weights using Gradient Descent: [4766724.90930426  821970.53061605  300286.5939388   696419.90394778]\n",
            "Epoch 384, Loss: 895585025475.1182\n",
            "Optimal weights using Gradient Descent: [4766724.95268828  821970.52355126  300286.42117423  696420.07585332]\n",
            "Epoch 385, Loss: 895585025469.0045\n",
            "Optimal weights using Gradient Descent: [4766724.99563846  821970.51648629  300286.24950322  696420.24670146]\n",
            "Epoch 386, Loss: 895585025462.9683\n",
            "Optimal weights using Gradient Descent: [4766725.03815914  821970.50942152  300286.0789188   696420.41649868]\n",
            "Epoch 387, Loss: 895585025457.0085\n",
            "Optimal weights using Gradient Descent: [4766725.08025462  821970.50235735  300285.90941405  696420.5852514 ]\n",
            "Epoch 388, Loss: 895585025451.124\n",
            "Optimal weights using Gradient Descent: [4766725.12192914  821970.49529416  300285.74098208  696420.75296602]\n",
            "Epoch 389, Loss: 895585025445.3142\n",
            "Optimal weights using Gradient Descent: [4766725.16318691  821970.4882323   300285.57361606  696420.9196489 ]\n",
            "Epoch 390, Loss: 895585025439.5778\n",
            "Optimal weights using Gradient Descent: [4766725.2040321   821970.48117216  300285.4073092   696421.08530636]\n",
            "Epoch 391, Loss: 895585025433.914\n",
            "Optimal weights using Gradient Descent: [4766725.24446885  821970.47411409  300285.24205475  696421.24994467]\n",
            "Epoch 392, Loss: 895585025428.3218\n",
            "Optimal weights using Gradient Descent: [4766725.28450122  821970.46705844  300285.07784601  696421.41357008]\n",
            "Epoch 393, Loss: 895585025422.8003\n",
            "Optimal weights using Gradient Descent: [4766725.32413327  821970.46000558  300284.91467632  696421.57618879]\n",
            "Epoch 394, Loss: 895585025417.3486\n",
            "Optimal weights using Gradient Descent: [4766725.363369    821970.45295584  300284.75253906  696421.73780695]\n",
            "Epoch 395, Loss: 895585025411.9661\n",
            "Optimal weights using Gradient Descent: [4766725.40221238  821970.44590956  300284.59142765  696421.89843071]\n",
            "Epoch 396, Loss: 895585025406.6514\n",
            "Optimal weights using Gradient Descent: [4766725.44066732  821970.43886707  300284.43133556  696422.05806615]\n",
            "Epoch 397, Loss: 895585025401.4039\n",
            "Optimal weights using Gradient Descent: [4766725.47873771  821970.43182872  300284.2722563   696422.21671932]\n",
            "Epoch 398, Loss: 895585025396.2229\n",
            "Optimal weights using Gradient Descent: [4766725.5164274   821970.42479481  300284.11418342  696422.37439624]\n",
            "Epoch 399, Loss: 895585025391.1072\n",
            "Optimal weights using Gradient Descent: [4766725.55374019  821970.41776567  300283.9571105   696422.5311029 ]\n",
            "Epoch 400, Loss: 895585025386.0564\n",
            "Optimal weights using Gradient Descent: [4766725.59067985  821970.41074162  300283.8010312   696422.68684522]\n",
            "Epoch 401, Loss: 895585025381.069\n",
            "Optimal weights using Gradient Descent: [4766725.62725011  821970.40372295  300283.64593916  696422.84162913]\n",
            "Epoch 402, Loss: 895585025376.1448\n",
            "Optimal weights using Gradient Descent: [4766725.66345468  821970.39670999  300283.49182813  696422.99546048]\n",
            "Epoch 403, Loss: 895585025371.2828\n",
            "Optimal weights using Gradient Descent: [4766725.6992972   821970.38970301  300283.33869183  696423.14834513]\n",
            "Epoch 404, Loss: 895585025366.4823\n",
            "Optimal weights using Gradient Descent: [4766725.73478129  821970.38270232  300283.18652408  696423.30028886]\n",
            "Epoch 405, Loss: 895585025361.7424\n",
            "Optimal weights using Gradient Descent: [4766725.76991054  821970.37570822  300283.0353187   696423.45129744]\n",
            "Epoch 406, Loss: 895585025357.0624\n",
            "Optimal weights using Gradient Descent: [4766725.8046885   821970.36872097  300282.88506958  696423.60137661]\n",
            "Epoch 407, Loss: 895585025352.4412\n",
            "Optimal weights using Gradient Descent: [4766725.83911868  821970.36174087  300282.73577062  696423.75053205]\n",
            "Epoch 408, Loss: 895585025347.8787\n",
            "Optimal weights using Gradient Descent: [4766725.87320455  821970.35476818  300282.58741577  696423.89876942]\n",
            "Epoch 409, Loss: 895585025343.3734\n",
            "Optimal weights using Gradient Descent: [4766725.90694957  821970.34780318  300282.43999904  696424.04609436]\n",
            "Epoch 410, Loss: 895585025338.9253\n",
            "Optimal weights using Gradient Descent: [4766725.94035714  821970.34084614  300282.29351444  696424.19251245]\n",
            "Epoch 411, Loss: 895585025334.5333\n",
            "Optimal weights using Gradient Descent: [4766725.97343063  821970.33389732  300282.14795605  696424.33802925]\n",
            "Epoch 412, Loss: 895585025330.1964\n",
            "Optimal weights using Gradient Descent: [4766726.00617339  821970.32695697  300282.00331797  696424.48265028]\n",
            "Epoch 413, Loss: 895585025325.9146\n",
            "Optimal weights using Gradient Descent: [4766726.03858872  821970.32002536  300281.85959435  696424.62638104]\n",
            "Epoch 414, Loss: 895585025321.6865\n",
            "Optimal weights using Gradient Descent: [4766726.0706799   821970.31310272  300281.71677937  696424.76922698]\n",
            "Epoch 415, Loss: 895585025317.5118\n",
            "Optimal weights using Gradient Descent: [4766726.10245016  821970.3061893   300281.57486724  696424.91119352]\n",
            "Epoch 416, Loss: 895585025313.39\n",
            "Optimal weights using Gradient Descent: [4766726.13390273  821970.29928534  300281.43385223  696425.05228605]\n",
            "Epoch 417, Loss: 895585025309.32\n",
            "Optimal weights using Gradient Descent: [4766726.16504076  821970.29239109  300281.29372864  696425.19250993]\n",
            "Epoch 418, Loss: 895585025305.3011\n",
            "Optimal weights using Gradient Descent: [4766726.19586742  821970.28550677  300281.15449077  696425.33187049]\n",
            "Epoch 419, Loss: 895585025301.3331\n",
            "Optimal weights using Gradient Descent: [4766726.22638581  821970.27863261  300281.01613302  696425.47037302]\n",
            "Epoch 420, Loss: 895585025297.4152\n",
            "Optimal weights using Gradient Descent: [4766726.25659902  821970.27176883  300280.87864976  696425.60802277]\n",
            "Epoch 421, Loss: 895585025293.5464\n",
            "Optimal weights using Gradient Descent: [4766726.28651009  821970.26491566  300280.74203546  696425.74482498]\n",
            "Epoch 422, Loss: 895585025289.7266\n",
            "Optimal weights using Gradient Descent: [4766726.31612205  821970.25807332  300280.60628457  696425.88078484]\n",
            "Epoch 423, Loss: 895585025285.9548\n",
            "Optimal weights using Gradient Descent: [4766726.3454379   821970.25124201  300280.4713916   696426.01590752]\n",
            "Epoch 424, Loss: 895585025282.2307\n",
            "Optimal weights using Gradient Descent: [4766726.37446058  821970.24442194  300280.33735112  696426.15019814]\n",
            "Epoch 425, Loss: 895585025278.5533\n",
            "Optimal weights using Gradient Descent: [4766726.40319304  821970.23761333  300280.20415768  696426.28366182]\n",
            "Epoch 426, Loss: 895585025274.9222\n",
            "Optimal weights using Gradient Descent: [4766726.43163817  821970.23081636  300280.0718059   696426.41630361]\n",
            "Epoch 427, Loss: 895585025271.337\n",
            "Optimal weights using Gradient Descent: [4766726.45979886  821970.22403125  300279.94029045  696426.54812857]\n",
            "Epoch 428, Loss: 895585025267.7971\n",
            "Optimal weights using Gradient Descent: [4766726.48767793  821970.21725818  300279.80960598  696426.6791417 ]\n",
            "Epoch 429, Loss: 895585025264.3014\n",
            "Optimal weights using Gradient Descent: [4766726.51527822  821970.21049734  300279.67974724  696426.80934797]\n",
            "Epoch 430, Loss: 895585025260.85\n",
            "Optimal weights using Gradient Descent: [4766726.5426025   821970.20374893  300279.55070896  696426.93875234]\n",
            "Epoch 431, Loss: 895585025257.4419\n",
            "Optimal weights using Gradient Descent: [4766726.56965354  821970.19701312  300279.42248593  696427.06735973]\n",
            "Epoch 432, Loss: 895585025254.0767\n",
            "Optimal weights using Gradient Descent: [4766726.59643407  821970.1902901   300279.29507296  696427.19517501]\n",
            "Epoch 433, Loss: 895585025250.7538\n",
            "Optimal weights using Gradient Descent: [4766726.62294679  821970.18358004  300279.16846492  696427.32220306]\n",
            "Epoch 434, Loss: 895585025247.4728\n",
            "Optimal weights using Gradient Descent: [4766726.64919439  821970.17688311  300279.04265668  696427.4484487 ]\n",
            "Epoch 435, Loss: 895585025244.2333\n",
            "Optimal weights using Gradient Descent: [4766726.67517951  821970.17019949  300278.91764316  696427.57391672]\n",
            "Epoch 436, Loss: 895585025241.0343\n",
            "Optimal weights using Gradient Descent: [4766726.70090478  821970.16352935  300278.79341931  696427.69861191]\n",
            "Epoch 437, Loss: 895585025237.8756\n",
            "Optimal weights using Gradient Descent: [4766726.72637279  821970.15687285  300278.66998012  696427.82253899]\n",
            "Epoch 438, Loss: 895585025234.7567\n",
            "Optimal weights using Gradient Descent: [4766726.75158613  821970.15023014  300278.54732059  696427.94570268]\n",
            "Epoch 439, Loss: 895585025231.677\n",
            "Optimal weights using Gradient Descent: [4766726.77654733  821970.14360138  300278.42543577  696428.06810766]\n",
            "Epoch 440, Loss: 895585025228.6361\n",
            "Optimal weights using Gradient Descent: [4766726.80125892  821970.13698674  300278.30432074  696428.1897586 ]\n",
            "Epoch 441, Loss: 895585025225.6333\n",
            "Optimal weights using Gradient Descent: [4766726.8257234   821970.13038636  300278.18397062  696428.31066011]\n",
            "Epoch 442, Loss: 895585025222.6685\n",
            "Optimal weights using Gradient Descent: [4766726.84994323  821970.12380038  300278.06438053  696428.43081679]\n",
            "Epoch 443, Loss: 895585025219.7406\n",
            "Optimal weights using Gradient Descent: [4766726.87392086  821970.11722896  300277.94554565  696428.55023321]\n",
            "Epoch 444, Loss: 895585025216.8499\n",
            "Optimal weights using Gradient Descent: [4766726.89765872  821970.11067223  300277.82746119  696428.66891392]\n",
            "Epoch 445, Loss: 895585025213.9954\n",
            "Optimal weights using Gradient Descent: [4766726.92115919  821970.10413034  300277.71012238  696428.78686342]\n",
            "Epoch 446, Loss: 895585025211.1766\n",
            "Optimal weights using Gradient Descent: [4766726.94442467  821970.09760342  300277.59352448  696428.90408622]\n",
            "Epoch 447, Loss: 895585025208.3934\n",
            "Optimal weights using Gradient Descent: [4766726.96745748  821970.09109161  300277.47766278  696429.02058675]\n",
            "Epoch 448, Loss: 895585025205.6453\n",
            "Optimal weights using Gradient Descent: [4766726.99025997  821970.08459503  300277.36253262  696429.13636947]\n",
            "Epoch 449, Loss: 895585025202.9316\n",
            "Optimal weights using Gradient Descent: [4766727.01283444  821970.07811381  300277.24812934  696429.25143876]\n",
            "Epoch 450, Loss: 895585025200.2518\n",
            "Optimal weights using Gradient Descent: [4766727.03518316  821970.07164808  300277.13444832  696429.36579902]\n",
            "Epoch 451, Loss: 895585025197.606\n",
            "Optimal weights using Gradient Descent: [4766727.05730839  821970.06519796  300277.02148499  696429.47945459]\n",
            "Epoch 452, Loss: 895585025194.9933\n",
            "Optimal weights using Gradient Descent: [4766727.07921237  821970.05876357  300276.90923479  696429.5924098 ]\n",
            "Epoch 453, Loss: 895585025192.4133\n",
            "Optimal weights using Gradient Descent: [4766727.10089731  821970.05234503  300276.79769318  696429.70466894]\n",
            "Epoch 454, Loss: 895585025189.8658\n",
            "Optimal weights using Gradient Descent: [4766727.1223654   821970.04594245  300276.68685567  696429.81623629]\n",
            "Epoch 455, Loss: 895585025187.3505\n",
            "Optimal weights using Gradient Descent: [4766727.14361881  821970.03955594  300276.57671779  696429.9271161 ]\n",
            "Epoch 456, Loss: 895585025184.8666\n",
            "Optimal weights using Gradient Descent: [4766727.16465969  821970.03318562  300276.46727509  696430.03731258]\n",
            "Epoch 457, Loss: 895585025182.414\n",
            "Optimal weights using Gradient Descent: [4766727.18549016  821970.02683158  300276.35852317  696430.14682993]\n",
            "Epoch 458, Loss: 895585025179.9921\n",
            "Optimal weights using Gradient Descent: [4766727.20611232  821970.02049394  300276.25045763  696430.25567232]\n",
            "Epoch 459, Loss: 895585025177.6005\n",
            "Optimal weights using Gradient Descent: [4766727.22652826  821970.01417279  300276.14307413  696430.36384389]\n",
            "Epoch 460, Loss: 895585025175.2391\n",
            "Optimal weights using Gradient Descent: [4766727.24674004  821970.00786824  300276.03636833  696430.47134877]\n",
            "Epoch 461, Loss: 895585025172.9075\n",
            "Optimal weights using Gradient Descent: [4766727.26674971  821970.00158037  300275.93033593  696430.57819104]\n",
            "Epoch 462, Loss: 895585025170.6047\n",
            "Optimal weights using Gradient Descent: [4766727.28655927  821969.9953093   300275.82497265  696430.68437476]\n",
            "Epoch 463, Loss: 895585025168.3313\n",
            "Optimal weights using Gradient Descent: [4766727.30617075  821969.9890551   300275.72027426  696430.789904  ]\n",
            "Epoch 464, Loss: 895585025166.086\n",
            "Optimal weights using Gradient Descent: [4766727.3255861   821969.98281787  300275.61623653  696430.89478276]\n",
            "Epoch 465, Loss: 895585025163.8691\n",
            "Optimal weights using Gradient Descent: [4766727.34480731  821969.97659769  300275.51285526  696430.99901504]\n",
            "Epoch 466, Loss: 895585025161.68\n",
            "Optimal weights using Gradient Descent: [4766727.3638363   821969.97039466  300275.41012631  696431.1026048 ]\n",
            "Epoch 467, Loss: 895585025159.5184\n",
            "Optimal weights using Gradient Descent: [4766727.382675    821969.96420885  300275.30804551  696431.205556  ]\n",
            "Epoch 468, Loss: 895585025157.3839\n",
            "Optimal weights using Gradient Descent: [4766727.40132531  821969.95804034  300275.20660877  696431.30787256]\n",
            "Epoch 469, Loss: 895585025155.2761\n",
            "Optimal weights using Gradient Descent: [4766727.41978912  821969.95188922  300275.10581199  696431.40955836]\n",
            "Epoch 470, Loss: 895585025153.1948\n",
            "Optimal weights using Gradient Descent: [4766727.4380683   821969.94575556  300275.00565112  696431.5106173 ]\n",
            "Epoch 471, Loss: 895585025151.1396\n",
            "Optimal weights using Gradient Descent: [4766727.45616468  821969.93963944  300274.90612212  696431.6110532 ]\n",
            "Epoch 472, Loss: 895585025149.1102\n",
            "Optimal weights using Gradient Descent: [4766727.4740801   821969.93354092  300274.80722098  696431.71086991]\n",
            "Epoch 473, Loss: 895585025147.1061\n",
            "Optimal weights using Gradient Descent: [4766727.49181636  821969.92746008  300274.70894373  696431.81007123]\n",
            "Epoch 474, Loss: 895585025145.1272\n",
            "Optimal weights using Gradient Descent: [4766727.50937526  821969.92139699  300274.6112864   696431.90866093]\n",
            "Epoch 475, Loss: 895585025143.1732\n",
            "Optimal weights using Gradient Descent: [4766727.52675857  821969.91535171  300274.51424506  696432.00664278]\n",
            "Epoch 476, Loss: 895585025141.2438\n",
            "Optimal weights using Gradient Descent: [4766727.54396805  821969.9093243   300274.4178158   696432.1040205 ]\n",
            "Epoch 477, Loss: 895585025139.3386\n",
            "Optimal weights using Gradient Descent: [4766727.56100543  821969.90331484  300274.32199475  696432.2007978 ]\n",
            "Epoch 478, Loss: 895585025137.4572\n",
            "Optimal weights using Gradient Descent: [4766727.57787244  821969.89732338  300274.22677804  696432.29697838]\n",
            "Epoch 479, Loss: 895585025135.599\n",
            "Optimal weights using Gradient Descent: [4766727.59457078  821969.89134998  300274.13216185  696432.39256591]\n",
            "Epoch 480, Loss: 895585025133.7645\n",
            "Optimal weights using Gradient Descent: [4766727.61110214  821969.88539469  300274.03814236  696432.48756402]\n",
            "Epoch 481, Loss: 895585025131.953\n",
            "Optimal weights using Gradient Descent: [4766727.62746818  821969.87945758  300273.9447158   696432.58197633]\n",
            "Epoch 482, Loss: 895585025130.1642\n",
            "Optimal weights using Gradient Descent: [4766727.64367057  821969.87353869  300273.85187839  696432.67580645]\n",
            "Epoch 483, Loss: 895585025128.3976\n",
            "Optimal weights using Gradient Descent: [4766727.65971092  821969.86763807  300273.75962642  696432.76905794]\n",
            "Epoch 484, Loss: 895585025126.6533\n",
            "Optimal weights using Gradient Descent: [4766727.67559088  821969.86175578  300273.66795616  696432.86173438]\n",
            "Epoch 485, Loss: 895585025124.9308\n",
            "Optimal weights using Gradient Descent: [4766727.69131203  821969.85589187  300273.57686392  696432.95383929]\n",
            "Epoch 486, Loss: 895585025123.2299\n",
            "Optimal weights using Gradient Descent: [4766727.70687598  821969.85004637  300273.48634605  696433.04537617]\n",
            "Epoch 487, Loss: 895585025121.5503\n",
            "Optimal weights using Gradient Descent: [4766727.72228428  821969.84421933  300273.39639891  696433.13634853]\n",
            "Epoch 488, Loss: 895585025119.8917\n",
            "Optimal weights using Gradient Descent: [4766727.7375385   821969.83841081  300273.30701886  696433.22675983]\n",
            "Epoch 489, Loss: 895585025118.2538\n",
            "Optimal weights using Gradient Descent: [4766727.75264018  821969.83262083  300273.21820233  696433.31661353]\n",
            "Epoch 490, Loss: 895585025116.6367\n",
            "Optimal weights using Gradient Descent: [4766727.76759085  821969.82684944  300273.12994574  696433.40591303]\n",
            "Epoch 491, Loss: 895585025115.0397\n",
            "Optimal weights using Gradient Descent: [4766727.782392    821969.82109667  300273.04224553  696433.49466176]\n",
            "Epoch 492, Loss: 895585025113.4625\n",
            "Optimal weights using Gradient Descent: [4766727.79704515  821969.81536256  300272.95509819  696433.58286309]\n",
            "Epoch 493, Loss: 895585025111.9053\n",
            "Optimal weights using Gradient Descent: [4766727.81155176  821969.80964715  300272.86850021  696433.6705204 ]\n",
            "Epoch 494, Loss: 895585025110.3674\n",
            "Optimal weights using Gradient Descent: [4766727.82591331  821969.80395047  300272.78244811  696433.75763702]\n",
            "Epoch 495, Loss: 895585025108.849\n",
            "Optimal weights using Gradient Descent: [4766727.84013124  821969.79827255  300272.69693843  696433.84421627]\n",
            "Epoch 496, Loss: 895585025107.3496\n",
            "Optimal weights using Gradient Descent: [4766727.85420699  821969.79261342  300272.61196773  696433.93026147]\n",
            "Epoch 497, Loss: 895585025105.8689\n",
            "Optimal weights using Gradient Descent: [4766727.86814198  821969.78697311  300272.52753259  696434.01577589]\n",
            "Epoch 498, Loss: 895585025104.4067\n",
            "Optimal weights using Gradient Descent: [4766727.88193763  821969.78135165  300272.44362963  696434.1007628 ]\n",
            "Epoch 499, Loss: 895585025102.9626\n",
            "Optimal weights using Gradient Descent: [4766727.89559531  821969.77574906  300272.36025548  696434.18522544]\n",
            "Epoch 500, Loss: 895585025101.537\n",
            "Optimal weights using Gradient Descent: [4766727.90911642  821969.77016536  300272.27740677  696434.26916702]\n",
            "Epoch 501, Loss: 895585025100.1289\n",
            "Optimal weights using Gradient Descent: [4766727.92250232  821969.76460059  300272.19508019  696434.35259077]\n",
            "Epoch 502, Loss: 895585025098.7385\n",
            "Optimal weights using Gradient Descent: [4766727.93575437  821969.75905475  300272.11327243  696434.43549985]\n",
            "Epoch 503, Loss: 895585025097.3656\n",
            "Optimal weights using Gradient Descent: [4766727.94887389  821969.75352788  300272.03198019  696434.51789743]\n",
            "Epoch 504, Loss: 895585025096.0098\n",
            "Optimal weights using Gradient Descent: [4766727.96186221  821969.74801999  300271.95120022  696434.59978666]\n",
            "Epoch 505, Loss: 895585025094.671\n",
            "Optimal weights using Gradient Descent: [4766727.97472065  821969.7425311   300271.87092927  696434.68117066]\n",
            "Epoch 506, Loss: 895585025093.349\n",
            "Optimal weights using Gradient Descent: [4766727.98745051  821969.73706122  300271.79116411  696434.76205254]\n",
            "Epoch 507, Loss: 895585025092.0435\n",
            "Optimal weights using Gradient Descent: [4766728.00005307  821969.73161037  300271.71190155  696434.84243539]\n",
            "Epoch 508, Loss: 895585025090.7542\n",
            "Optimal weights using Gradient Descent: [4766728.01252961  821969.72617856  300271.6331384   696434.92232227]\n",
            "Epoch 509, Loss: 895585025089.4812\n",
            "Optimal weights using Gradient Descent: [4766728.02488137  821969.72076581  300271.55487149  696435.00171624]\n",
            "Epoch 510, Loss: 895585025088.224\n",
            "Optimal weights using Gradient Descent: [4766728.03710962  821969.71537213  300271.47709769  696435.08062032]\n",
            "Epoch 511, Loss: 895585025086.9827\n",
            "Optimal weights using Gradient Descent: [4766728.04921559  821969.70999752  300271.39981387  696435.15903753]\n",
            "Epoch 512, Loss: 895585025085.7567\n",
            "Optimal weights using Gradient Descent: [4766728.0612005   821969.70464199  300271.32301692  696435.23697086]\n",
            "Epoch 513, Loss: 895585025084.546\n",
            "Optimal weights using Gradient Descent: [4766728.07306556  821969.69930556  300271.24670378  696435.31442329]\n",
            "Epoch 514, Loss: 895585025083.3507\n",
            "Optimal weights using Gradient Descent: [4766728.08481197  821969.69398823  300271.17087137  696435.39139778]\n",
            "Epoch 515, Loss: 895585025082.1703\n",
            "Optimal weights using Gradient Descent: [4766728.09644091  821969.68869     300271.09551665  696435.46789726]\n",
            "Epoch 516, Loss: 895585025081.0045\n",
            "Optimal weights using Gradient Descent: [4766728.10795357  821969.68341088  300271.02063659  696435.54392465]\n",
            "Epoch 517, Loss: 895585025079.8534\n",
            "Optimal weights using Gradient Descent: [4766728.11935109  821969.67815088  300270.9462282   696435.61948287]\n",
            "Epoch 518, Loss: 895585025078.7166\n",
            "Optimal weights using Gradient Descent: [4766728.13063465  821969.67290998  300270.87228848  696435.69457479]\n",
            "Epoch 519, Loss: 895585025077.5942\n",
            "Optimal weights using Gradient Descent: [4766728.14180537  821969.6676882   300270.79881447  696435.76920328]\n",
            "Epoch 520, Loss: 895585025076.4857\n",
            "Optimal weights using Gradient Descent: [4766728.15286438  821969.66248554  300270.72580322  696435.8433712 ]\n",
            "Epoch 521, Loss: 895585025075.391\n",
            "Optimal weights using Gradient Descent: [4766728.1638128   821969.65730199  300270.6532518   696435.91708138]\n",
            "Epoch 522, Loss: 895585025074.3099\n",
            "Optimal weights using Gradient Descent: [4766728.17465173  821969.65213755  300270.5811573   696435.99033663]\n",
            "Epoch 523, Loss: 895585025073.2426\n",
            "Optimal weights using Gradient Descent: [4766728.18538228  821969.64699222  300270.50951683  696436.06313975]\n",
            "Epoch 524, Loss: 895585025072.1885\n",
            "Optimal weights using Gradient Descent: [4766728.19600552  821969.64186599  300270.43832752  696436.13549352]\n",
            "Epoch 525, Loss: 895585025071.1476\n",
            "Optimal weights using Gradient Descent: [4766728.20652253  821969.63675886  300270.36758651  696436.20740071]\n",
            "Epoch 526, Loss: 895585025070.1195\n",
            "Optimal weights using Gradient Descent: [4766728.21693437  821969.63167083  300270.29729096  696436.27886406]\n",
            "Epoch 527, Loss: 895585025069.1046\n",
            "Optimal weights using Gradient Descent: [4766728.22724209  821969.62660187  300270.22743806  696436.34988631]\n",
            "Epoch 528, Loss: 895585025068.1022\n",
            "Optimal weights using Gradient Descent: [4766728.23744673  821969.621552    300270.158025    696436.42047017]\n",
            "Epoch 529, Loss: 895585025067.1122\n",
            "Optimal weights using Gradient Descent: [4766728.24754933  821969.61652119  300270.08904901  696436.49061834]\n",
            "Epoch 530, Loss: 895585025066.1346\n",
            "Optimal weights using Gradient Descent: [4766728.2575509   821969.61150944  300270.02050731  696436.56033349]\n",
            "Epoch 531, Loss: 895585025065.1694\n",
            "Optimal weights using Gradient Descent: [4766728.26745246  821969.60651674  300269.95239716  696436.62961829]\n",
            "Epoch 532, Loss: 895585025064.2162\n",
            "Optimal weights using Gradient Descent: [4766728.277255    821969.60154308  300269.88471582  696436.6984754 ]\n",
            "Epoch 533, Loss: 895585025063.2748\n",
            "Optimal weights using Gradient Descent: [4766728.28695951  821969.59658843  300269.8174606   696436.76690743]\n",
            "Epoch 534, Loss: 895585025062.3453\n",
            "Optimal weights using Gradient Descent: [4766728.29656698  821969.5916528   300269.75062878  696436.83491701]\n",
            "Epoch 535, Loss: 895585025061.4274\n",
            "Optimal weights using Gradient Descent: [4766728.30607837  821969.58673616  300269.68421769  696436.90250674]\n",
            "Epoch 536, Loss: 895585025060.5209\n",
            "Optimal weights using Gradient Descent: [4766728.31549465  821969.5818385   300269.61822468  696436.9696792 ]\n",
            "Epoch 537, Loss: 895585025059.6257\n",
            "Optimal weights using Gradient Descent: [4766728.32481677  821969.57695981  300269.55264709  696437.03643695]\n",
            "Epoch 538, Loss: 895585025058.7417\n",
            "Optimal weights using Gradient Descent: [4766728.33404567  821969.57210007  300269.4874823   696437.10278256]\n",
            "Epoch 539, Loss: 895585025057.8687\n",
            "Optimal weights using Gradient Descent: [4766728.34318228  821969.56725925  300269.42272769  696437.16871856]\n",
            "Epoch 540, Loss: 895585025057.0065\n",
            "Optimal weights using Gradient Descent: [4766728.35222752  821969.56243735  300269.35838068  696437.23424746]\n",
            "Epoch 541, Loss: 895585025056.1553\n",
            "Optimal weights using Gradient Descent: [4766728.36118231  821969.55763434  300269.29443869  696437.29937177]\n",
            "Epoch 542, Loss: 895585025055.3147\n",
            "Optimal weights using Gradient Descent: [4766728.37004755  821969.5528502   300269.23089915  696437.36409399]\n",
            "Epoch 543, Loss: 895585025054.4846\n",
            "Optimal weights using Gradient Descent: [4766728.37882414  821969.54808491  300269.16775952  696437.42841659]\n",
            "Epoch 544, Loss: 895585025053.6647\n",
            "Optimal weights using Gradient Descent: [4766728.38751296  821969.54333846  300269.10501727  696437.49234202]\n",
            "Epoch 545, Loss: 895585025052.8552\n",
            "Optimal weights using Gradient Descent: [4766728.39611489  821969.53861082  300269.04266988  696437.55587274]\n",
            "Epoch 546, Loss: 895585025052.0559\n",
            "Optimal weights using Gradient Descent: [4766728.40463081  821969.53390196  300268.98071487  696437.61901117]\n",
            "Epoch 547, Loss: 895585025051.2662\n",
            "Optimal weights using Gradient Descent: [4766728.41306156  821969.52921186  300268.91914975  696437.68175973]\n",
            "Epoch 548, Loss: 895585025050.4866\n",
            "Optimal weights using Gradient Descent: [4766728.42140801  821969.52454051  300268.85797206  696437.74412082]\n",
            "Epoch 549, Loss: 895585025049.7167\n",
            "Optimal weights using Gradient Descent: [4766728.429671    821969.51988787  300268.79717934  696437.80609682]\n",
            "Epoch 550, Loss: 895585025048.9564\n",
            "Optimal weights using Gradient Descent: [4766728.43785135  821969.51525392  300268.73676916  696437.8676901 ]\n",
            "Epoch 551, Loss: 895585025048.2056\n",
            "Optimal weights using Gradient Descent: [4766728.4459499   821969.51063863  300268.67673911  696437.92890302]\n",
            "Epoch 552, Loss: 895585025047.4642\n",
            "Optimal weights using Gradient Descent: [4766728.45396747  821969.50604198  300268.61708678  696437.98973793]\n",
            "Epoch 553, Loss: 895585025046.7322\n",
            "Optimal weights using Gradient Descent: [4766728.46190486  821969.50146394  300268.55780978  696438.05019714]\n",
            "Epoch 554, Loss: 895585025046.009\n",
            "Optimal weights using Gradient Descent: [4766728.46976287  821969.49690448  300268.49890575  696438.11028298]\n",
            "Epoch 555, Loss: 895585025045.295\n",
            "Optimal weights using Gradient Descent: [4766728.47754231  821969.49236358  300268.44037231  696438.16999773]\n",
            "Epoch 556, Loss: 895585025044.59\n",
            "Optimal weights using Gradient Descent: [4766728.48524395  821969.48784119  300268.38220714  696438.22934369]\n",
            "Epoch 557, Loss: 895585025043.8937\n",
            "Optimal weights using Gradient Descent: [4766728.49286857  821969.48333731  300268.32440791  696438.28832313]\n",
            "Epoch 558, Loss: 895585025043.2062\n",
            "Optimal weights using Gradient Descent: [4766728.50041695  821969.47885188  300268.26697229  696438.3469383 ]\n",
            "Epoch 559, Loss: 895585025042.5269\n",
            "Optimal weights using Gradient Descent: [4766728.50788985  821969.47438489  300268.209898    696438.40519145]\n",
            "Epoch 560, Loss: 895585025041.8564\n",
            "Optimal weights using Gradient Descent: [4766728.51528801  821969.46993631  300268.15318275  696438.46308481]\n",
            "Epoch 561, Loss: 895585025041.1941\n",
            "Optimal weights using Gradient Descent: [4766728.5226122   821969.46550609  300268.09682427  696438.52062058]\n",
            "Epoch 562, Loss: 895585025040.5402\n",
            "Optimal weights using Gradient Descent: [4766728.52986314  821969.46109421  300268.04082031  696438.57780098]\n",
            "Epoch 563, Loss: 895585025039.8944\n",
            "Optimal weights using Gradient Descent: [4766728.53704157  821969.45670064  300267.98516862  696438.63462819]\n",
            "Epoch 564, Loss: 895585025039.2567\n",
            "Optimal weights using Gradient Descent: [4766728.54414822  821969.45232533  300267.92986699  696438.69110439]\n",
            "Epoch 565, Loss: 895585025038.6268\n",
            "Optimal weights using Gradient Descent: [4766728.5511838   821969.44796827  300267.87491319  696438.74723174]\n",
            "Epoch 566, Loss: 895585025038.005\n",
            "Optimal weights using Gradient Descent: [4766728.55814903  821969.44362941  300267.82030504  696438.80301238]\n",
            "Epoch 567, Loss: 895585025037.3909\n",
            "Optimal weights using Gradient Descent: [4766728.5650446   821969.43930872  300267.76604035  696438.85844846]\n",
            "Epoch 568, Loss: 895585025036.7844\n",
            "Optimal weights using Gradient Descent: [4766728.57187122  821969.43500616  300267.71211694  696438.91354209]\n",
            "Epoch 569, Loss: 895585025036.1854\n",
            "Optimal weights using Gradient Descent: [4766728.57862957  821969.4307217   300267.65853267  696438.96829539]\n",
            "Epoch 570, Loss: 895585025035.5941\n",
            "Optimal weights using Gradient Descent: [4766728.58532034  821969.42645529  300267.60528539  696439.02271045]\n",
            "Epoch 571, Loss: 895585025035.0099\n",
            "Optimal weights using Gradient Descent: [4766728.5919442   821969.42220692  300267.55237297  696439.07678935]\n",
            "Epoch 572, Loss: 895585025034.4331\n",
            "Optimal weights using Gradient Descent: [4766728.59850182  821969.41797653  300267.49979329  696439.13053417]\n",
            "Epoch 573, Loss: 895585025033.8635\n",
            "Optimal weights using Gradient Descent: [4766728.60499387  821969.41376409  300267.44754426  696439.18394696]\n",
            "Epoch 574, Loss: 895585025033.301\n",
            "Optimal weights using Gradient Descent: [4766728.611421    821969.40956956  300267.39562379  696439.23702977]\n",
            "Epoch 575, Loss: 895585025032.7455\n",
            "Optimal weights using Gradient Descent: [4766728.61778385  821969.40539291  300267.3440298   696439.28978463]\n",
            "Epoch 576, Loss: 895585025032.1969\n",
            "Optimal weights using Gradient Descent: [4766728.62408308  821969.40123409  300267.29276023  696439.34221356]\n",
            "Epoch 577, Loss: 895585025031.6552\n",
            "Optimal weights using Gradient Descent: [4766728.63031931  821969.39709306  300267.24181303  696439.39431857]\n",
            "Epoch 578, Loss: 895585025031.1202\n",
            "Optimal weights using Gradient Descent: [4766728.63649318  821969.39296979  300267.19118617  696439.44610165]\n",
            "Epoch 579, Loss: 895585025030.5919\n",
            "Optimal weights using Gradient Descent: [4766728.64260531  821969.38886424  300267.14087762  696439.49756479]\n",
            "Epoch 580, Loss: 895585025030.0702\n",
            "Optimal weights using Gradient Descent: [4766728.64865632  821969.38477636  300267.09088537  696439.54870996]\n",
            "Epoch 581, Loss: 895585025029.5548\n",
            "Optimal weights using Gradient Descent: [4766728.65464683  821969.38070612  300267.04120742  696439.59953912]\n",
            "Epoch 582, Loss: 895585025029.0461\n",
            "Optimal weights using Gradient Descent: [4766728.66057742  821969.37665347  300266.9918418   696439.65005422]\n",
            "Epoch 583, Loss: 895585025028.5436\n",
            "Optimal weights using Gradient Descent: [4766728.66644871  821969.37261837  300266.94278652  696439.70025719]\n",
            "Epoch 584, Loss: 895585025028.0475\n",
            "Optimal weights using Gradient Descent: [4766728.67226129  821969.36860078  300266.89403964  696439.75014995]\n",
            "Epoch 585, Loss: 895585025027.5575\n",
            "Optimal weights using Gradient Descent: [4766728.67801574  821969.36460066  300266.8455992   696439.79973441]\n",
            "Epoch 586, Loss: 895585025027.0736\n",
            "Optimal weights using Gradient Descent: [4766728.68371265  821969.36061796  300266.79746326  696439.84901248]\n",
            "Epoch 587, Loss: 895585025026.5957\n",
            "Optimal weights using Gradient Descent: [4766728.68935258  821969.35665265  300266.74962991  696439.89798605]\n",
            "Epoch 588, Loss: 895585025026.1237\n",
            "Optimal weights using Gradient Descent: [4766728.69493612  821969.35270468  300266.70209724  696439.94665698]\n",
            "Epoch 589, Loss: 895585025025.6576\n",
            "Optimal weights using Gradient Descent: [4766728.70046383  821969.34877401  300266.65486334  696439.99502715]\n",
            "Epoch 590, Loss: 895585025025.1975\n",
            "Optimal weights using Gradient Descent: [4766728.70593625  821969.34486058  300266.60792634  696440.0430984 ]\n",
            "Epoch 591, Loss: 895585025024.743\n",
            "Optimal weights using Gradient Descent: [4766728.71135395  821969.34096437  300266.56128435  696440.09087259]\n",
            "Epoch 592, Loss: 895585025024.2941\n",
            "Optimal weights using Gradient Descent: [4766728.71671748  821969.33708532  300266.51493552  696440.13835154]\n",
            "Epoch 593, Loss: 895585025023.8507\n",
            "Optimal weights using Gradient Descent: [4766728.72202737  821969.33322339  300266.46887799  696440.18553706]\n",
            "Epoch 594, Loss: 895585025023.4132\n",
            "Optimal weights using Gradient Descent: [4766728.72728416  821969.32937854  300266.42310993  696440.23243098]\n",
            "Epoch 595, Loss: 895585025022.981\n",
            "Optimal weights using Gradient Descent: [4766728.73248838  821969.32555072  300266.37762951  696440.27903508]\n",
            "Epoch 596, Loss: 895585025022.5541\n",
            "Optimal weights using Gradient Descent: [4766728.73764056  821969.32173988  300266.33243492  696440.32535116]\n",
            "Epoch 597, Loss: 895585025022.1324\n",
            "Optimal weights using Gradient Descent: [4766728.74274122  821969.31794598  300266.28752435  696440.37138098]\n",
            "Epoch 598, Loss: 895585025021.7162\n",
            "Optimal weights using Gradient Descent: [4766728.74779087  821969.31416898  300266.242896    696440.41712632]\n",
            "Epoch 599, Loss: 895585025021.3049\n",
            "Optimal weights using Gradient Descent: [4766728.75279003  821969.31040882  300266.19854811  696440.46258892]\n",
            "Epoch 600, Loss: 895585025020.8992\n",
            "Optimal weights using Gradient Descent: [4766728.75773919  821969.30666546  300266.15447889  696440.50777053]\n",
            "Epoch 601, Loss: 895585025020.498\n",
            "Optimal weights using Gradient Descent: [4766728.76263886  821969.30293886  300266.11068659  696440.55267288]\n",
            "Epoch 602, Loss: 895585025020.1022\n",
            "Optimal weights using Gradient Descent: [4766728.76748954  821969.29922897  300266.06716947  696440.59729769]\n",
            "Epoch 603, Loss: 895585025019.711\n",
            "Optimal weights using Gradient Descent: [4766728.77229171  821969.29553574  300266.02392578  696440.64164668]\n",
            "Epoch 604, Loss: 895585025019.3251\n",
            "Optimal weights using Gradient Descent: [4766728.77704586  821969.29185912  300265.9809538   696440.68572153]\n",
            "Epoch 605, Loss: 895585025018.9436\n",
            "Optimal weights using Gradient Descent: [4766728.78175246  821969.28819907  300265.93825182  696440.72952396]\n",
            "Epoch 606, Loss: 895585025018.567\n",
            "Optimal weights using Gradient Descent: [4766728.786412    821969.28455554  300265.89581814  696440.77305562]\n",
            "Epoch 607, Loss: 895585025018.1951\n",
            "Optimal weights using Gradient Descent: [4766728.79102495  821969.28092848  300265.85365105  696440.8163182 ]\n",
            "Epoch 608, Loss: 895585025017.828\n",
            "Optimal weights using Gradient Descent: [4766728.79559176  821969.27731784  300265.81174889  696440.85931335]\n",
            "Epoch 609, Loss: 895585025017.4653\n",
            "Optimal weights using Gradient Descent: [4766728.80011291  821969.27372358  300265.77010997  696440.90204272]\n",
            "Epoch 610, Loss: 895585025017.1069\n",
            "Optimal weights using Gradient Descent: [4766728.80458884  821969.27014564  300265.72873265  696440.94450795]\n",
            "Epoch 611, Loss: 895585025016.7534\n",
            "Optimal weights using Gradient Descent: [4766728.80902002  821969.26658399  300265.68761526  696440.98671066]\n",
            "Epoch 612, Loss: 895585025016.404\n",
            "Optimal weights using Gradient Descent: [4766728.81340688  821969.26303856  300265.64675617  696441.02865248]\n",
            "Epoch 613, Loss: 895585025016.0592\n",
            "Optimal weights using Gradient Descent: [4766728.81774988  821969.25950931  300265.60615375  696441.07033502]\n",
            "Epoch 614, Loss: 895585025015.7185\n",
            "Optimal weights using Gradient Descent: [4766728.82204944  821969.2559962   300265.56580639  696441.11175987]\n",
            "Epoch 615, Loss: 895585025015.382\n",
            "Optimal weights using Gradient Descent: [4766728.82630601  821969.25249917  300265.52571246  696441.15292862]\n",
            "Epoch 616, Loss: 895585025015.0498\n",
            "Optimal weights using Gradient Descent: [4766728.83052002  821969.24901817  300265.48587038  696441.19384285]\n",
            "Epoch 617, Loss: 895585025014.7216\n",
            "Optimal weights using Gradient Descent: [4766728.83469188  821969.24555316  300265.44627856  696441.23450414]\n",
            "Epoch 618, Loss: 895585025014.3977\n",
            "Optimal weights using Gradient Descent: [4766728.83882203  821969.24210408  300265.40693541  696441.27491403]\n",
            "Epoch 619, Loss: 895585025014.0776\n",
            "Optimal weights using Gradient Descent: [4766728.84291087  821969.23867089  300265.36783938  696441.31507409]\n",
            "Epoch 620, Loss: 895585025013.7615\n",
            "Optimal weights using Gradient Descent: [4766728.84695883  821969.23525354  300265.32898889  696441.35498585]\n",
            "Epoch 621, Loss: 895585025013.4496\n",
            "Optimal weights using Gradient Descent: [4766728.8509663   821969.23185196  300265.29038241  696441.39465084]\n",
            "Epoch 622, Loss: 895585025013.1415\n",
            "Optimal weights using Gradient Descent: [4766728.8549337   821969.22846613  300265.2520184   696441.43407059]\n",
            "Epoch 623, Loss: 895585025012.8372\n",
            "Optimal weights using Gradient Descent: [4766728.85886143  821969.22509598  300265.21389532  696441.4732466 ]\n",
            "Epoch 624, Loss: 895585025012.5365\n",
            "Optimal weights using Gradient Descent: [4766728.86274988  821969.22174146  300265.17601166  696441.51218039]\n",
            "Epoch 625, Loss: 895585025012.2397\n",
            "Optimal weights using Gradient Descent: [4766728.86659945  821969.21840253  300265.13836591  696441.55087344]\n",
            "Epoch 626, Loss: 895585025011.9467\n",
            "Optimal weights using Gradient Descent: [4766728.87041051  821969.21507914  300265.10095656  696441.58932724]\n",
            "Epoch 627, Loss: 895585025011.6572\n",
            "Optimal weights using Gradient Descent: [4766728.87418347  821969.21177122  300265.06378214  696441.62754326]\n",
            "Epoch 628, Loss: 895585025011.3713\n",
            "Optimal weights using Gradient Descent: [4766728.8779187   821969.20847875  300265.02684115  696441.66552298]\n",
            "Epoch 629, Loss: 895585025011.089\n",
            "Optimal weights using Gradient Descent: [4766728.88161658  821969.20520165  300264.99013213  696441.70326785]\n",
            "Epoch 630, Loss: 895585025010.8103\n",
            "Optimal weights using Gradient Descent: [4766728.88527748  821969.20193988  300264.95365361  696441.74077931]\n",
            "Epoch 631, Loss: 895585025010.5349\n",
            "Optimal weights using Gradient Descent: [4766728.88890177  821969.1986934   300264.91740414  696441.77805881]\n",
            "Epoch 632, Loss: 895585025010.2631\n",
            "Optimal weights using Gradient Descent: [4766728.89248982  821969.19546214  300264.88138228  696441.81510778]\n",
            "Epoch 633, Loss: 895585025009.9945\n",
            "Optimal weights using Gradient Descent: [4766728.89604198  821969.19224607  300264.84558659  696441.85192765]\n",
            "Epoch 634, Loss: 895585025009.7292\n",
            "Optimal weights using Gradient Descent: [4766728.89955863  821969.18904512  300264.81001564  696441.88851981]\n",
            "Epoch 635, Loss: 895585025009.4674\n",
            "Optimal weights using Gradient Descent: [4766728.9030401   821969.18585925  300264.77466803  696441.92488569]\n",
            "Epoch 636, Loss: 895585025009.2089\n",
            "Optimal weights using Gradient Descent: [4766728.90648677  821969.1826884   300264.73954235  696441.96102667]\n",
            "Epoch 637, Loss: 895585025008.9535\n",
            "Optimal weights using Gradient Descent: [4766728.90989896  821969.17953253  300264.70463718  696441.99694414]\n",
            "Epoch 638, Loss: 895585025008.7013\n",
            "Optimal weights using Gradient Descent: [4766728.91327704  821969.17639158  300264.66995115  696442.03263949]\n",
            "Epoch 639, Loss: 895585025008.4521\n",
            "Optimal weights using Gradient Descent: [4766728.91662133  821969.1732655   300264.63548288  696442.06811407]\n",
            "Epoch 640, Loss: 895585025008.2063\n",
            "Optimal weights using Gradient Descent: [4766728.91993218  821969.17015424  300264.60123099  696442.10336926]\n",
            "Epoch 641, Loss: 895585025007.9631\n",
            "Optimal weights using Gradient Descent: [4766728.92320992  821969.16705775  300264.56719411  696442.13840641]\n",
            "Epoch 642, Loss: 895585025007.7234\n",
            "Optimal weights using Gradient Descent: [4766728.92645489  821969.16397598  300264.5333709   696442.17322686]\n",
            "Epoch 643, Loss: 895585025007.4866\n",
            "Optimal weights using Gradient Descent: [4766728.9296674   821969.16090887  300264.49976001  696442.20783195]\n",
            "Epoch 644, Loss: 895585025007.2526\n",
            "Optimal weights using Gradient Descent: [4766728.9328478   821969.15785638  300264.4663601   696442.242223  ]\n",
            "Epoch 645, Loss: 895585025007.0215\n",
            "Optimal weights using Gradient Descent: [4766728.93599638  821969.15481845  300264.43316983  696442.27640135]\n",
            "Epoch 646, Loss: 895585025006.7933\n",
            "Optimal weights using Gradient Descent: [4766728.93911348  821969.15179502  300264.4001879   696442.3103683 ]\n",
            "Epoch 647, Loss: 895585025006.568\n",
            "Optimal weights using Gradient Descent: [4766728.94219941  821969.14878606  300264.36741299  696442.34412515]\n",
            "Epoch 648, Loss: 895585025006.3455\n",
            "Optimal weights using Gradient Descent: [4766728.94525448  821969.14579151  300264.33484379  696442.37767321]\n",
            "Epoch 649, Loss: 895585025006.1256\n",
            "Optimal weights using Gradient Descent: [4766728.948279    821969.14281131  300264.302479    696442.41101376]\n",
            "Epoch 650, Loss: 895585025005.9088\n",
            "Optimal weights using Gradient Descent: [4766728.95127327  821969.13984541  300264.27031735  696442.44414808]\n",
            "Epoch 651, Loss: 895585025005.6942\n",
            "Optimal weights using Gradient Descent: [4766728.95423761  821969.13689377  300264.23835754  696442.47707744]\n",
            "Epoch 652, Loss: 895585025005.4828\n",
            "Optimal weights using Gradient Descent: [4766728.95717229  821969.13395633  300264.20659832  696442.50980311]\n",
            "Epoch 653, Loss: 895585025005.2737\n",
            "Optimal weights using Gradient Descent: [4766728.96007764  821969.13103303  300264.17503841  696442.54232635]\n",
            "Epoch 654, Loss: 895585025005.0671\n",
            "Optimal weights using Gradient Descent: [4766728.96295392  821969.12812383  300264.14367656  696442.57464841]\n",
            "Epoch 655, Loss: 895585025004.8633\n",
            "Optimal weights using Gradient Descent: [4766728.96580145  821969.12522868  300264.11251152  696442.60677052]\n",
            "Epoch 656, Loss: 895585025004.662\n",
            "Optimal weights using Gradient Descent: [4766728.9686205   821969.12234752  300264.08154206  696442.63869392]\n",
            "Epoch 657, Loss: 895585025004.4631\n",
            "Optimal weights using Gradient Descent: [4766728.97141136  821969.11948031  300264.05076693  696442.67041984]\n",
            "Epoch 658, Loss: 895585025004.2668\n",
            "Optimal weights using Gradient Descent: [4766728.97417431  821969.11662698  300264.02018493  696442.7019495 ]\n",
            "Epoch 659, Loss: 895585025004.0729\n",
            "Optimal weights using Gradient Descent: [4766728.97690963  821969.11378749  300263.98979482  696442.7332841 ]\n",
            "Epoch 660, Loss: 895585025003.8815\n",
            "Optimal weights using Gradient Descent: [4766728.9796176   821969.11096179  300263.95959541  696442.76442485]\n",
            "Epoch 661, Loss: 895585025003.6924\n",
            "Optimal weights using Gradient Descent: [4766728.98229849  821969.10814983  300263.92958549  696442.79537296]\n",
            "Epoch 662, Loss: 895585025003.5055\n",
            "Optimal weights using Gradient Descent: [4766728.98495257  821969.10535155  300263.89976388  696442.8261296 ]\n",
            "Epoch 663, Loss: 895585025003.3212\n",
            "Optimal weights using Gradient Descent: [4766728.9875801   821969.1025669   300263.87012937  696442.85669596]\n",
            "Epoch 664, Loss: 895585025003.1388\n",
            "Optimal weights using Gradient Descent: [4766728.99018137  821969.09979583  300263.8406808   696442.88707321]\n",
            "Epoch 665, Loss: 895585025002.9591\n",
            "Optimal weights using Gradient Descent: [4766728.99275662  821969.09703829  300263.81141699  696442.91726252]\n",
            "Epoch 666, Loss: 895585025002.7812\n",
            "Optimal weights using Gradient Descent: [4766728.99530612  821969.09429422  300263.78233678  696442.94726506]\n",
            "Epoch 667, Loss: 895585025002.606\n",
            "Optimal weights using Gradient Descent: [4766728.99783012  821969.09156359  300263.75343902  696442.97708197]\n",
            "Epoch 668, Loss: 895585025002.4327\n",
            "Optimal weights using Gradient Descent: [4766729.00032888  821969.08884632  300263.72472256  696443.0067144 ]\n",
            "Epoch 669, Loss: 895585025002.2617\n",
            "Optimal weights using Gradient Descent: [4766729.00280266  821969.08614238  300263.69618625  696443.03616349]\n",
            "Epoch 670, Loss: 895585025002.0927\n",
            "Optimal weights using Gradient Descent: [4766729.00525169  821969.08345172  300263.66782896  696443.06543037]\n",
            "Epoch 671, Loss: 895585025001.9258\n",
            "Optimal weights using Gradient Descent: [4766729.00767624  821969.08077427  300263.63964957  696443.09451616]\n",
            "Epoch 672, Loss: 895585025001.761\n",
            "Optimal weights using Gradient Descent: [4766729.01007654  821969.07810999  300263.61164695  696443.12342199]\n",
            "Epoch 673, Loss: 895585025001.5983\n",
            "Optimal weights using Gradient Descent: [4766729.01245284  821969.07545883  300263.58381999  696443.15214896]\n",
            "Epoch 674, Loss: 895585025001.4375\n",
            "Optimal weights using Gradient Descent: [4766729.01480538  821969.07282074  300263.55616759  696443.18069818]\n",
            "Epoch 675, Loss: 895585025001.2788\n",
            "Optimal weights using Gradient Descent: [4766729.01713439  821969.07019566  300263.52868865  696443.20907075]\n",
            "Epoch 676, Loss: 895585025001.1221\n",
            "Optimal weights using Gradient Descent: [4766729.01944011  821969.06758354  300263.50138207  696443.23726775]\n",
            "Epoch 677, Loss: 895585025000.9673\n",
            "Optimal weights using Gradient Descent: [4766729.02172277  821969.06498434  300263.47424677  696443.26529028]\n",
            "Epoch 678, Loss: 895585025000.8142\n",
            "Optimal weights using Gradient Descent: [4766729.02398261  821969.06239801  300263.44728167  696443.2931394 ]\n",
            "Epoch 679, Loss: 895585025000.6635\n",
            "Optimal weights using Gradient Descent: [4766729.02621985  821969.05982448  300263.42048571  696443.3208162 ]\n",
            "Epoch 680, Loss: 895585025000.5143\n",
            "Optimal weights using Gradient Descent: [4766729.02843471  821969.05726372  300263.39385781  696443.34832172]\n",
            "Epoch 681, Loss: 895585025000.3672\n",
            "Optimal weights using Gradient Descent: [4766729.03062743  821969.05471566  300263.36739692  696443.37565704]\n",
            "Epoch 682, Loss: 895585025000.2216\n",
            "Optimal weights using Gradient Descent: [4766729.03279822  821969.05218026  300263.34110199  696443.4028232 ]\n",
            "Epoch 683, Loss: 895585025000.0779\n",
            "Optimal weights using Gradient Descent: [4766729.0349473   821969.04965748  300263.31497198  696443.42982125]\n",
            "Epoch 684, Loss: 895585024999.9362\n",
            "Optimal weights using Gradient Descent: [4766729.03707489  821969.04714725  300263.28900584  696443.45665221]\n",
            "Epoch 685, Loss: 895585024999.7961\n",
            "Optimal weights using Gradient Descent: [4766729.03918121  821969.04464952  300263.26320254  696443.48331714]\n",
            "Epoch 686, Loss: 895585024999.6578\n",
            "Optimal weights using Gradient Descent: [4766729.04126646  821969.04216426  300263.23756107  696443.50981705]\n",
            "Epoch 687, Loss: 895585024999.5212\n",
            "Optimal weights using Gradient Descent: [4766729.04333086  821969.0396914   300263.21208039  696443.53615295]\n",
            "Epoch 688, Loss: 895585024999.3864\n",
            "Optimal weights using Gradient Descent: [4766729.04537461  821969.0372309   300263.18675951  696443.56232587]\n",
            "Epoch 689, Loss: 895585024999.253\n",
            "Optimal weights using Gradient Descent: [4766729.04739793  821969.0347827   300263.16159741  696443.58833682]\n",
            "Epoch 690, Loss: 895585024999.1213\n",
            "Optimal weights using Gradient Descent: [4766729.04940102  821969.03234676  300263.1365931   696443.61418678]\n",
            "Epoch 691, Loss: 895585024998.9916\n",
            "Optimal weights using Gradient Descent: [4766729.05138407  821969.02992303  300263.11174558  696443.63987675]\n",
            "Epoch 692, Loss: 895585024998.8634\n",
            "Optimal weights using Gradient Descent: [4766729.05334729  821969.02751145  300263.08705387  696443.66540773]\n",
            "Epoch 693, Loss: 895585024998.7366\n",
            "Optimal weights using Gradient Descent: [4766729.05529089  821969.02511198  300263.06251699  696443.69078069]\n",
            "Epoch 694, Loss: 895585024998.6115\n",
            "Optimal weights using Gradient Descent: [4766729.05721504  821969.02272456  300263.03813395  696443.71599662]\n",
            "Epoch 695, Loss: 895585024998.4878\n",
            "Optimal weights using Gradient Descent: [4766729.05911995  821969.02034915  300263.01390381  696443.74105647]\n",
            "Epoch 696, Loss: 895585024998.3656\n",
            "Optimal weights using Gradient Descent: [4766729.06100582  821969.0179857   300262.98982558  696443.76596122]\n",
            "Epoch 697, Loss: 895585024998.2452\n",
            "Optimal weights using Gradient Descent: [4766729.06287283  821969.01563415  300262.96589833  696443.79071182]\n",
            "Epoch 698, Loss: 895585024998.1262\n",
            "Optimal weights using Gradient Descent: [4766729.06472116  821969.01329446  300262.94212109  696443.81530922]\n",
            "Epoch 699, Loss: 895585024998.0088\n",
            "Optimal weights using Gradient Descent: [4766729.06655101  821969.01096658  300262.91849292  696443.83975438]\n",
            "Epoch 700, Loss: 895585024997.8925\n",
            "Optimal weights using Gradient Descent: [4766729.06836257  821969.00865046  300262.89501289  696443.86404823]\n",
            "Epoch 701, Loss: 895585024997.778\n",
            "Optimal weights using Gradient Descent: [4766729.07015601  821969.00634605  300262.87168007  696443.8881917 ]\n",
            "Epoch 702, Loss: 895585024997.6647\n",
            "Optimal weights using Gradient Descent: [4766729.07193151  821969.0040533   300262.84849353  696443.91218573]\n",
            "Epoch 703, Loss: 895585024997.5529\n",
            "Optimal weights using Gradient Descent: [4766729.07368926  821969.00177216  300262.82545234  696443.93603123]\n",
            "Epoch 704, Loss: 895585024997.4425\n",
            "Optimal weights using Gradient Descent: [4766729.07542943  821968.99950258  300262.8025556   696443.95972914]\n",
            "Epoch 705, Loss: 895585024997.3333\n",
            "Optimal weights using Gradient Descent: [4766729.0771522   821968.99724452  300262.7798024   696443.98328035]\n",
            "Epoch 706, Loss: 895585024997.2255\n",
            "Optimal weights using Gradient Descent: [4766729.07885774  821968.99499793  300262.75719183  696444.00668577]\n",
            "Epoch 707, Loss: 895585024997.1193\n",
            "Optimal weights using Gradient Descent: [4766729.08054623  821968.99276275  300262.73472299  696444.02994631]\n",
            "Epoch 708, Loss: 895585024997.014\n",
            "Optimal weights using Gradient Descent: [4766729.08221783  821968.99053894  300262.712395    696444.05306285]\n",
            "Epoch 709, Loss: 895585024996.9104\n",
            "Optimal weights using Gradient Descent: [4766729.08387272  821968.98832644  300262.69020698  696444.0760363 ]\n",
            "Epoch 710, Loss: 895585024996.8079\n",
            "Optimal weights using Gradient Descent: [4766729.08551105  821968.98612522  300262.66815803  696444.09886752]\n",
            "Epoch 711, Loss: 895585024996.7069\n",
            "Optimal weights using Gradient Descent: [4766729.08713301  821968.98393523  300262.64624729  696444.12155741]\n",
            "Epoch 712, Loss: 895585024996.6069\n",
            "Optimal weights using Gradient Descent: [4766729.08873874  821968.98175641  300262.62447388  696444.14410684]\n",
            "Epoch 713, Loss: 895585024996.5082\n",
            "Optimal weights using Gradient Descent: [4766729.09032842  821968.97958871  300262.60283695  696444.16651667]\n",
            "Epoch 714, Loss: 895585024996.4108\n",
            "Optimal weights using Gradient Descent: [4766729.0919022   821968.9774321   300262.58133563  696444.18878776]\n",
            "Epoch 715, Loss: 895585024996.3145\n",
            "Optimal weights using Gradient Descent: [4766729.09346024  821968.97528652  300262.55996907  696444.21092097]\n",
            "Epoch 716, Loss: 895585024996.2194\n",
            "Optimal weights using Gradient Descent: [4766729.0950027   821968.97315192  300262.53873643  696444.23291716]\n",
            "Epoch 717, Loss: 895585024996.1257\n",
            "Optimal weights using Gradient Descent: [4766729.09652974  821968.97102826  300262.51763686  696444.25477717]\n",
            "Epoch 718, Loss: 895585024996.0328\n",
            "Optimal weights using Gradient Descent: [4766729.09804151  821968.96891548  300262.49666953  696444.27650184]\n",
            "Epoch 719, Loss: 895585024995.9413\n",
            "Optimal weights using Gradient Descent: [4766729.09953816  821968.96681355  300262.4758336   696444.29809202]\n",
            "Epoch 720, Loss: 895585024995.8508\n",
            "Optimal weights using Gradient Descent: [4766729.10101984  821968.96472242  300262.45512825  696444.31954852]\n",
            "Epoch 721, Loss: 895585024995.7616\n",
            "Optimal weights using Gradient Descent: [4766729.1024867   821968.96264203  300262.43455266  696444.34087217]\n",
            "Epoch 722, Loss: 895585024995.6735\n",
            "Optimal weights using Gradient Descent: [4766729.1039389   821968.96057234  300262.41410601  696444.36206381]\n",
            "Epoch 723, Loss: 895585024995.5864\n",
            "Optimal weights using Gradient Descent: [4766729.10537658  821968.9585133   300262.3937875   696444.38312423]\n",
            "Epoch 724, Loss: 895585024995.5002\n",
            "Optimal weights using Gradient Descent: [4766729.10679988  821968.95646488  300262.37359631  696444.40405426]\n",
            "Epoch 725, Loss: 895585024995.4155\n",
            "Optimal weights using Gradient Descent: [4766729.10820894  821968.95442701  300262.35353165  696444.4248547 ]\n",
            "Epoch 726, Loss: 895585024995.3315\n",
            "Optimal weights using Gradient Descent: [4766729.10960392  821968.95239966  300262.33359271  696444.44552635]\n",
            "Epoch 727, Loss: 895585024995.2488\n",
            "Optimal weights using Gradient Descent: [4766729.11098494  821968.95038277  300262.31377872  696444.46607   ]\n",
            "Epoch 728, Loss: 895585024995.1669\n",
            "Optimal weights using Gradient Descent: [4766729.11235216  821968.94837631  300262.29408889  696444.48648645]\n",
            "Epoch 729, Loss: 895585024995.0862\n",
            "Optimal weights using Gradient Descent: [4766729.1137057   821968.94638022  300262.27452243  696444.50677649]\n",
            "Epoch 730, Loss: 895585024995.0063\n",
            "Optimal weights using Gradient Descent: [4766729.11504571  821968.94439446  300262.25507857  696444.52694089]\n",
            "Epoch 731, Loss: 895585024994.9276\n",
            "Optimal weights using Gradient Descent: [4766729.11637231  821968.94241898  300262.23575654  696444.54698043]\n",
            "Epoch 732, Loss: 895585024994.8496\n",
            "Optimal weights using Gradient Descent: [4766729.11768565  821968.94045374  300262.21655557  696444.56689588]\n",
            "Epoch 733, Loss: 895585024994.773\n",
            "Optimal weights using Gradient Descent: [4766729.11898586  821968.93849869  300262.1974749   696444.58668801]\n",
            "Epoch 734, Loss: 895585024994.697\n",
            "Optimal weights using Gradient Descent: [4766729.12027307  821968.93655379  300262.17851379  696444.60635758]\n",
            "Epoch 735, Loss: 895585024994.6221\n",
            "Optimal weights using Gradient Descent: [4766729.1215474   821968.93461899  300262.15967147  696444.62590535]\n",
            "Epoch 736, Loss: 895585024994.5481\n",
            "Optimal weights using Gradient Descent: [4766729.12280899  821968.93269425  300262.1409472   696444.64533208]\n",
            "Epoch 737, Loss: 895585024994.4749\n",
            "Optimal weights using Gradient Descent: [4766729.12405796  821968.93077952  300262.12234023  696444.6646385 ]\n",
            "Epoch 738, Loss: 895585024994.4027\n",
            "Optimal weights using Gradient Descent: [4766729.12529445  821968.92887475  300262.10384984  696444.68382537]\n",
            "Epoch 739, Loss: 895585024994.3314\n",
            "Optimal weights using Gradient Descent: [4766729.12651857  821968.9269799   300262.08547528  696444.70289342]\n",
            "Epoch 740, Loss: 895585024994.2611\n",
            "Optimal weights using Gradient Descent: [4766729.12773045  821968.92509493  300262.06721584  696444.72184339]\n",
            "Epoch 741, Loss: 895585024994.1915\n",
            "Optimal weights using Gradient Descent: [4766729.12893021  821968.92321979  300262.04907078  696444.740676  ]\n",
            "Epoch 742, Loss: 895585024994.1228\n",
            "Optimal weights using Gradient Descent: [4766729.13011797  821968.92135444  300262.03103939  696444.75939199]\n",
            "Epoch 743, Loss: 895585024994.055\n",
            "Optimal weights using Gradient Descent: [4766729.13129385  821968.91949883  300262.01312095  696444.77799207]\n",
            "Epoch 744, Loss: 895585024993.988\n",
            "Optimal weights using Gradient Descent: [4766729.13245798  821968.91765292  300261.99531476  696444.79647696]\n",
            "Epoch 745, Loss: 895585024993.922\n",
            "Optimal weights using Gradient Descent: [4766729.13361046  821968.91581666  300261.97762011  696444.81484737]\n",
            "Epoch 746, Loss: 895585024993.8566\n",
            "Optimal weights using Gradient Descent: [4766729.13475142  821968.91399001  300261.9600363   696444.83310402]\n",
            "Epoch 747, Loss: 895585024993.7921\n",
            "Optimal weights using Gradient Descent: [4766729.13588097  821968.91217293  300261.94256262  696444.85124759]\n",
            "Epoch 748, Loss: 895585024993.7283\n",
            "Optimal weights using Gradient Descent: [4766729.13699923  821968.91036538  300261.9251984   696444.8692788 ]\n",
            "Epoch 749, Loss: 895585024993.6654\n",
            "Optimal weights using Gradient Descent: [4766729.1381063   821968.9085673   300261.90794295  696444.88719833]\n",
            "Epoch 750, Loss: 895585024993.6034\n",
            "Optimal weights using Gradient Descent: [4766729.1392023   821968.90677865  300261.89079557  696444.90500688]\n",
            "Epoch 751, Loss: 895585024993.542\n",
            "Optimal weights using Gradient Descent: [4766729.14028734  821968.9049994   300261.87375559  696444.92270514]\n",
            "Epoch 752, Loss: 895585024993.4811\n",
            "Optimal weights using Gradient Descent: [4766729.14136153  821968.9032295   300261.85682234  696444.94029378]\n",
            "Epoch 753, Loss: 895585024993.4215\n",
            "Optimal weights using Gradient Descent: [4766729.14242498  821968.90146891  300261.83999515  696444.95777348]\n",
            "Epoch 754, Loss: 895585024993.3624\n",
            "Optimal weights using Gradient Descent: [4766729.1434778   821968.89971758  300261.82327335  696444.97514492]\n",
            "Epoch 755, Loss: 895585024993.304\n",
            "Optimal weights using Gradient Descent: [4766729.14452008  821968.89797547  300261.80665627  696444.99240876]\n",
            "Epoch 756, Loss: 895585024993.2463\n",
            "Optimal weights using Gradient Descent: [4766729.14555195  821968.89624253  300261.79014327  696445.00956568]\n",
            "Epoch 757, Loss: 895585024993.1893\n",
            "Optimal weights using Gradient Descent: [4766729.14657349  821968.89451874  300261.77373368  696445.02661633]\n",
            "Epoch 758, Loss: 895585024993.1332\n",
            "Optimal weights using Gradient Descent: [4766729.14758482  821968.89280403  300261.75742686  696445.04356136]\n",
            "Epoch 759, Loss: 895585024993.0776\n",
            "Optimal weights using Gradient Descent: [4766729.14858604  821968.89109838  300261.74122216  696445.06040144]\n",
            "Epoch 760, Loss: 895585024993.0227\n",
            "Optimal weights using Gradient Descent: [4766729.14957724  821968.88940173  300261.72511894  696445.07713721]\n",
            "Epoch 761, Loss: 895585024992.9688\n",
            "Optimal weights using Gradient Descent: [4766729.15055853  821968.88771406  300261.70911656  696445.09376932]\n",
            "Epoch 762, Loss: 895585024992.915\n",
            "Optimal weights using Gradient Descent: [4766729.15153001  821968.8860353   300261.69321439  696445.1102984 ]\n",
            "Epoch 763, Loss: 895585024992.8623\n",
            "Optimal weights using Gradient Descent: [4766729.15249178  821968.88436543  300261.67741181  696445.1267251 ]\n",
            "Epoch 764, Loss: 895585024992.8102\n",
            "Optimal weights using Gradient Descent: [4766729.15344392  821968.88270441  300261.66170818  696445.14305004]\n",
            "Epoch 765, Loss: 895585024992.7588\n",
            "Optimal weights using Gradient Descent: [4766729.15438655  821968.88105218  300261.64610288  696445.15927386]\n",
            "Epoch 766, Loss: 895585024992.708\n",
            "Optimal weights using Gradient Descent: [4766729.15531975  821968.87940871  300261.6305953   696445.17539718]\n",
            "Epoch 767, Loss: 895585024992.6575\n",
            "Optimal weights using Gradient Descent: [4766729.15624361  821968.87777396  300261.61518483  696445.19142063]\n",
            "Epoch 768, Loss: 895585024992.6078\n",
            "Optimal weights using Gradient Descent: [4766729.15715824  821968.87614789  300261.59987084  696445.20734482]\n",
            "Epoch 769, Loss: 895585024992.559\n",
            "Optimal weights using Gradient Descent: [4766729.15806372  821968.87453045  300261.58465275  696445.22317036]\n",
            "Epoch 770, Loss: 895585024992.5105\n",
            "Optimal weights using Gradient Descent: [4766729.15896015  821968.87292161  300261.56952994  696445.23889786]\n",
            "Epoch 771, Loss: 895585024992.4628\n",
            "Optimal weights using Gradient Descent: [4766729.15984761  821968.87132133  300261.55450182  696445.25452794]\n",
            "Epoch 772, Loss: 895585024992.4155\n",
            "Optimal weights using Gradient Descent: [4766729.1607262   821968.86972956  300261.53956779  696445.27006119]\n",
            "Epoch 773, Loss: 895585024992.369\n",
            "Optimal weights using Gradient Descent: [4766729.161596    821968.86814626  300261.52472726  696445.28549821]\n",
            "Epoch 774, Loss: 895585024992.3229\n",
            "Optimal weights using Gradient Descent: [4766729.16245711  821968.8665714   300261.50997965  696445.3008396 ]\n",
            "Epoch 775, Loss: 895585024992.2775\n",
            "Optimal weights using Gradient Descent: [4766729.1633096   821968.86500493  300261.49532437  696445.31608595]\n",
            "Epoch 776, Loss: 895585024992.2327\n",
            "Optimal weights using Gradient Descent: [4766729.16415357  821968.86344681  300261.48076084  696445.33123785]\n",
            "Epoch 777, Loss: 895585024992.1882\n",
            "Optimal weights using Gradient Descent: [4766729.1649891   821968.86189701  300261.46628849  696445.34629588]\n",
            "Epoch 778, Loss: 895585024992.1443\n",
            "Optimal weights using Gradient Descent: [4766729.16581627  821968.86035549  300261.45190674  696445.36126062]\n",
            "Epoch 779, Loss: 895585024992.1012\n",
            "Optimal weights using Gradient Descent: [4766729.16663517  821968.8588222   300261.43761502  696445.37613264]\n",
            "Epoch 780, Loss: 895585024992.0583\n",
            "Optimal weights using Gradient Descent: [4766729.16744589  821968.8572971   300261.42341278  696445.39091253]\n",
            "Epoch 781, Loss: 895585024992.0164\n",
            "Optimal weights using Gradient Descent: [4766729.16824849  821968.85578017  300261.40929945  696445.40560085]\n",
            "Epoch 782, Loss: 895585024991.9746\n",
            "Optimal weights using Gradient Descent: [4766729.16904307  821968.85427135  300261.39527446  696445.42019817]\n",
            "Epoch 783, Loss: 895585024991.9336\n",
            "Optimal weights using Gradient Descent: [4766729.1698297   821968.85277061  300261.38133728  696445.43470505]\n",
            "Epoch 784, Loss: 895585024991.8927\n",
            "Optimal weights using Gradient Descent: [4766729.17060847  821968.85127791  300261.36748733  696445.44912206]\n",
            "Epoch 785, Loss: 895585024991.8528\n",
            "Optimal weights using Gradient Descent: [4766729.17137945  821968.84979321  300261.35372409  696445.46344973]\n",
            "Epoch 786, Loss: 895585024991.8131\n",
            "Optimal weights using Gradient Descent: [4766729.17214272  821968.84831648  300261.34004699  696445.47768864]\n",
            "Epoch 787, Loss: 895585024991.774\n",
            "Optimal weights using Gradient Descent: [4766729.17289836  821968.84684767  300261.32645551  696445.49183932]\n",
            "Epoch 788, Loss: 895585024991.7352\n",
            "Optimal weights using Gradient Descent: [4766729.17364644  821968.84538674  300261.31294911  696445.50590233]\n",
            "Epoch 789, Loss: 895585024991.697\n",
            "Optimal weights using Gradient Descent: [4766729.17438704  821968.84393367  300261.29952725  696445.51987821]\n",
            "Epoch 790, Loss: 895585024991.6594\n",
            "Optimal weights using Gradient Descent: [4766729.17512023  821968.8424884   300261.28618939  696445.53376749]\n",
            "Epoch 791, Loss: 895585024991.6222\n",
            "Optimal weights using Gradient Descent: [4766729.17584609  821968.84105091  300261.27293503  696445.54757071]\n",
            "Epoch 792, Loss: 895585024991.5853\n",
            "Optimal weights using Gradient Descent: [4766729.1765647   821968.83962116  300261.25976362  696445.5612884 ]\n",
            "Epoch 793, Loss: 895585024991.5491\n",
            "Optimal weights using Gradient Descent: [4766729.17727611  821968.8381991   300261.24667465  696445.5749211 ]\n",
            "Epoch 794, Loss: 895585024991.5133\n",
            "Optimal weights using Gradient Descent: [4766729.17798042  821968.8367847   300261.2336676   696445.58846932]\n",
            "Epoch 795, Loss: 895585024991.4778\n",
            "Optimal weights using Gradient Descent: [4766729.17867768  821968.83537793  300261.22074196  696445.6019336 ]\n",
            "Epoch 796, Loss: 895585024991.4429\n",
            "Optimal weights using Gradient Descent: [4766729.17936796  821968.83397874  300261.20789721  696445.61531444]\n",
            "Epoch 797, Loss: 895585024991.4084\n",
            "Optimal weights using Gradient Descent: [4766729.18005135  821968.8325871   300261.19513286  696445.62861237]\n",
            "Epoch 798, Loss: 895585024991.3743\n",
            "Optimal weights using Gradient Descent: [4766729.1807279   821968.83120297  300261.18244839  696445.6418279 ]\n",
            "Epoch 799, Loss: 895585024991.3406\n",
            "Optimal weights using Gradient Descent: [4766729.18139768  821968.82982632  300261.16984331  696445.65496154]\n",
            "Epoch 800, Loss: 895585024991.3073\n",
            "Optimal weights using Gradient Descent: [4766729.18206077  821968.82845711  300261.15731711  696445.6680138 ]\n",
            "Epoch 801, Loss: 895585024991.2744\n",
            "Optimal weights using Gradient Descent: [4766729.18271723  821968.8270953   300261.1448693   696445.68098517]\n",
            "Epoch 802, Loss: 895585024991.2421\n",
            "Optimal weights using Gradient Descent: [4766729.18336712  821968.82574086  300261.13249939  696445.69387617]\n",
            "Epoch 803, Loss: 895585024991.2098\n",
            "Optimal weights using Gradient Descent: [4766729.18401051  821968.82439374  300261.12020689  696445.70668728]\n",
            "Epoch 804, Loss: 895585024991.1782\n",
            "Optimal weights using Gradient Descent: [4766729.18464747  821968.82305392  300261.10799131  696445.719419  ]\n",
            "Epoch 805, Loss: 895585024991.147\n",
            "Optimal weights using Gradient Descent: [4766729.18527806  821968.82172137  300261.09585218  696445.73207182]\n",
            "Epoch 806, Loss: 895585024991.1162\n",
            "Optimal weights using Gradient Descent: [4766729.18590234  821968.82039603  300261.08378901  696445.74464624]\n",
            "Epoch 807, Loss: 895585024991.0857\n",
            "Optimal weights using Gradient Descent: [4766729.18652039  821968.81907788  300261.07180133  696445.75714273]\n",
            "Epoch 808, Loss: 895585024991.0555\n",
            "Optimal weights using Gradient Descent: [4766729.18713225  821968.81776688  300261.05988866  696445.76956178]\n",
            "Epoch 809, Loss: 895585024991.026\n",
            "Optimal weights using Gradient Descent: [4766729.18773799  821968.816463    300261.04805053  696445.78190386]\n",
            "Epoch 810, Loss: 895585024990.9965\n",
            "Optimal weights using Gradient Descent: [4766729.18833767  821968.8151662   300261.03628647  696445.79416946]\n",
            "Epoch 811, Loss: 895585024990.9674\n",
            "Optimal weights using Gradient Descent: [4766729.18893136  821968.81387645  300261.02459603  696445.80635905]\n",
            "Epoch 812, Loss: 895585024990.9388\n",
            "Optimal weights using Gradient Descent: [4766729.18951911  821968.81259371  300261.01297873  696445.81847309]\n",
            "Epoch 813, Loss: 895585024990.9106\n",
            "Optimal weights using Gradient Descent: [4766729.19010098  821968.81131795  300261.00143412  696445.83051206]\n",
            "Epoch 814, Loss: 895585024990.8826\n",
            "Optimal weights using Gradient Descent: [4766729.19067704  821968.81004913  300260.98996175  696445.84247641]\n",
            "Epoch 815, Loss: 895585024990.8551\n",
            "Optimal weights using Gradient Descent: [4766729.19124733  821968.80878722  300260.97856115  696445.85436662]\n",
            "Epoch 816, Loss: 895585024990.8278\n",
            "Optimal weights using Gradient Descent: [4766729.19181192  821968.80753218  300260.96723189  696445.86618314]\n",
            "Epoch 817, Loss: 895585024990.8009\n",
            "Optimal weights using Gradient Descent: [4766729.19237087  821968.80628398  300260.95597351  696445.87792642]\n",
            "Epoch 818, Loss: 895585024990.7744\n",
            "Optimal weights using Gradient Descent: [4766729.19292422  821968.80504258  300260.94478556  696445.88959692]\n",
            "Epoch 819, Loss: 895585024990.7482\n",
            "Optimal weights using Gradient Descent: [4766729.19347204  821968.80380796  300260.93366762  696445.90119509]\n",
            "Epoch 820, Loss: 895585024990.7222\n",
            "Optimal weights using Gradient Descent: [4766729.19401439  821968.80258008  300260.92261923  696445.91272137]\n",
            "Epoch 821, Loss: 895585024990.6967\n",
            "Optimal weights using Gradient Descent: [4766729.19455131  821968.8013589   300260.91163996  696445.92417622]\n",
            "Epoch 822, Loss: 895585024990.6713\n",
            "Optimal weights using Gradient Descent: [4766729.19508286  821968.80014439  300260.90072938  696445.93556007]\n",
            "Epoch 823, Loss: 895585024990.6464\n",
            "Optimal weights using Gradient Descent: [4766729.1956091   821968.79893652  300260.88988705  696445.94687336]\n",
            "Epoch 824, Loss: 895585024990.6217\n",
            "Optimal weights using Gradient Descent: [4766729.19613007  821968.79773525  300260.87911256  696445.95811654]\n",
            "Epoch 825, Loss: 895585024990.5973\n",
            "Optimal weights using Gradient Descent: [4766729.19664583  821968.79654055  300260.86840547  696445.96929003]\n",
            "Epoch 826, Loss: 895585024990.5735\n",
            "Optimal weights using Gradient Descent: [4766729.19715644  821968.79535239  300260.85776536  696445.98039426]\n",
            "Epoch 827, Loss: 895585024990.5496\n",
            "Optimal weights using Gradient Descent: [4766729.19766194  821968.79417074  300260.84719181  696445.99142967]\n",
            "Epoch 828, Loss: 895585024990.5262\n",
            "Optimal weights using Gradient Descent: [4766729.19816238  821968.79299556  300260.8366844   696446.00239667]\n",
            "Epoch 829, Loss: 895585024990.5029\n",
            "Optimal weights using Gradient Descent: [4766729.19865782  821968.79182682  300260.82624273  696446.0132957 ]\n",
            "Epoch 830, Loss: 895585024990.48\n",
            "Optimal weights using Gradient Descent: [4766729.19914831  821968.79066449  300260.81586637  696446.02412717]\n",
            "Epoch 831, Loss: 895585024990.4574\n",
            "Optimal weights using Gradient Descent: [4766729.19963389  821968.78950853  300260.80555491  696446.03489151]\n",
            "Epoch 832, Loss: 895585024990.4352\n",
            "Optimal weights using Gradient Descent: [4766729.20011462  821968.78835892  300260.79530796  696446.04558912]\n",
            "Epoch 833, Loss: 895585024990.4132\n",
            "Optimal weights using Gradient Descent: [4766729.20059053  821968.78721562  300260.78512511  696446.05622042]\n",
            "Epoch 834, Loss: 895585024990.3914\n",
            "Optimal weights using Gradient Descent: [4766729.20106169  821968.78607859  300260.77500594  696446.06678583]\n",
            "Epoch 835, Loss: 895585024990.3699\n",
            "Optimal weights using Gradient Descent: [4766729.20152814  821968.78494782  300260.76495008  696446.07728574]\n",
            "Epoch 836, Loss: 895585024990.3486\n",
            "Optimal weights using Gradient Descent: [4766729.20198992  821968.78382326  300260.75495711  696446.08772057]\n",
            "Epoch 837, Loss: 895585024990.3278\n",
            "Optimal weights using Gradient Descent: [4766729.20244709  821968.78270488  300260.74502664  696446.09809071]\n",
            "Epoch 838, Loss: 895585024990.307\n",
            "Optimal weights using Gradient Descent: [4766729.20289968  821968.78159266  300260.73515828  696446.10839657]\n",
            "Epoch 839, Loss: 895585024990.2866\n",
            "Optimal weights using Gradient Descent: [4766729.20334775  821968.78048656  300260.72535165  696446.11863855]\n",
            "Epoch 840, Loss: 895585024990.2666\n",
            "Optimal weights using Gradient Descent: [4766729.20379134  821968.77938656  300260.71560636  696446.12881703]\n",
            "Epoch 841, Loss: 895585024990.2463\n",
            "Optimal weights using Gradient Descent: [4766729.20423049  821968.77829261  300260.70592201  696446.13893242]\n",
            "Epoch 842, Loss: 895585024990.2267\n",
            "Optimal weights using Gradient Descent: [4766729.20466525  821968.77720469  300260.69629823  696446.14898511]\n",
            "Epoch 843, Loss: 895585024990.2074\n",
            "Optimal weights using Gradient Descent: [4766729.20509566  821968.77612277  300260.68673465  696446.15897548]\n",
            "Epoch 844, Loss: 895585024990.1882\n",
            "Optimal weights using Gradient Descent: [4766729.20552177  821968.77504682  300260.67723087  696446.16890391]\n",
            "Epoch 845, Loss: 895585024990.1692\n",
            "Optimal weights using Gradient Descent: [4766729.20594361  821968.7739768   300260.66778653  696446.1787708 ]\n",
            "Epoch 846, Loss: 895585024990.1505\n",
            "Optimal weights using Gradient Descent: [4766729.20636124  821968.7729127   300260.65840126  696446.18857652]\n",
            "Epoch 847, Loss: 895585024990.132\n",
            "Optimal weights using Gradient Descent: [4766729.20677469  821968.77185447  300260.64907468  696446.19832145]\n",
            "Epoch 848, Loss: 895585024990.1135\n",
            "Optimal weights using Gradient Descent: [4766729.20718401  821968.77080209  300260.63980642  696446.20800597]\n",
            "Epoch 849, Loss: 895585024990.0957\n",
            "Optimal weights using Gradient Descent: [4766729.20758923  821968.76975552  300260.63059613  696446.21763045]\n",
            "Epoch 850, Loss: 895585024990.0779\n",
            "Optimal weights using Gradient Descent: [4766729.20799041  821968.76871474  300260.62144343  696446.22719526]\n",
            "Epoch 851, Loss: 895585024990.0602\n",
            "Optimal weights using Gradient Descent: [4766729.20838757  821968.76767972  300260.61234798  696446.23670078]\n",
            "Epoch 852, Loss: 895585024990.0427\n",
            "Optimal weights using Gradient Descent: [4766729.20878075  821968.76665043  300260.60330939  696446.24614737]\n",
            "Epoch 853, Loss: 895585024990.0258\n",
            "Optimal weights using Gradient Descent: [4766729.20917001  821968.76562684  300260.59432733  696446.25553539]\n",
            "Epoch 854, Loss: 895585024990.0088\n",
            "Optimal weights using Gradient Descent: [4766729.20955538  821968.76460892  300260.58540144  696446.26486521]\n",
            "Epoch 855, Loss: 895585024989.9921\n",
            "Optimal weights using Gradient Descent: [4766729.20993689  821968.76359663  300260.57653136  696446.27413718]\n",
            "Epoch 856, Loss: 895585024989.9755\n",
            "Optimal weights using Gradient Descent: [4766729.21031458  821968.76258996  300260.56771675  696446.28335168]\n",
            "Epoch 857, Loss: 895585024989.959\n",
            "Optimal weights using Gradient Descent: [4766729.2106885   821968.76158888  300260.55895726  696446.29250904]\n",
            "Epoch 858, Loss: 895585024989.943\n",
            "Optimal weights using Gradient Descent: [4766729.21105868  821968.76059334  300260.55025253  696446.30160963]\n",
            "Epoch 859, Loss: 895585024989.9271\n",
            "Optimal weights using Gradient Descent: [4766729.21142516  821968.75960333  300260.54160224  696446.3106538 ]\n",
            "Epoch 860, Loss: 895585024989.9113\n",
            "Optimal weights using Gradient Descent: [4766729.21178797  821968.75861882  300260.53300604  696446.31964189]\n",
            "Epoch 861, Loss: 895585024989.8959\n",
            "Optimal weights using Gradient Descent: [4766729.21214715  821968.75763978  300260.52446358  696446.32857426]\n",
            "Epoch 862, Loss: 895585024989.8806\n",
            "Optimal weights using Gradient Descent: [4766729.21250275  821968.75666618  300260.51597454  696446.33745124]\n",
            "Epoch 863, Loss: 895585024989.8654\n",
            "Optimal weights using Gradient Descent: [4766729.21285478  821968.75569798  300260.50753857  696446.34627319]\n",
            "Epoch 864, Loss: 895585024989.8502\n",
            "Optimal weights using Gradient Descent: [4766729.2132033   821968.75473518  300260.49915535  696446.35504044]\n",
            "Epoch 865, Loss: 895585024989.8356\n",
            "Optimal weights using Gradient Descent: [4766729.21354833  821968.75377773  300260.49082454  696446.36375332]\n",
            "Epoch 866, Loss: 895585024989.821\n",
            "Optimal weights using Gradient Descent: [4766729.21388991  821968.7528256   300260.48254582  696446.37241219]\n",
            "Epoch 867, Loss: 895585024989.8066\n",
            "Optimal weights using Gradient Descent: [4766729.21422808  821968.75187878  300260.47431886  696446.38101737]\n",
            "Epoch 868, Loss: 895585024989.7925\n",
            "Optimal weights using Gradient Descent: [4766729.21456286  821968.75093723  300260.46614334  696446.38956919]\n",
            "Epoch 869, Loss: 895585024989.7783\n",
            "Optimal weights using Gradient Descent: [4766729.2148943   821968.75000093  300260.45801893  696446.39806799]\n",
            "Epoch 870, Loss: 895585024989.7645\n",
            "Optimal weights using Gradient Descent: [4766729.21522242  821968.74906985  300260.44994531  696446.40651409]\n",
            "Epoch 871, Loss: 895585024989.7507\n",
            "Optimal weights using Gradient Descent: [4766729.21554726  821968.74814396  300260.44192216  696446.41490783]\n",
            "Epoch 872, Loss: 895585024989.7372\n",
            "Optimal weights using Gradient Descent: [4766729.21586885  821968.74722323  300260.43394918  696446.42324951]\n",
            "Epoch 873, Loss: 895585024989.7239\n",
            "Optimal weights using Gradient Descent: [4766729.21618722  821968.74630764  300260.42602604  696446.43153947]\n",
            "Epoch 874, Loss: 895585024989.7107\n",
            "Optimal weights using Gradient Descent: [4766729.21650242  821968.74539716  300260.41815243  696446.43977803]\n",
            "Epoch 875, Loss: 895585024989.6978\n",
            "Optimal weights using Gradient Descent: [4766729.21681446  821968.74449177  300260.41032804  696446.4479655 ]\n",
            "Epoch 876, Loss: 895585024989.6847\n",
            "Optimal weights using Gradient Descent: [4766729.21712338  821968.74359143  300260.40255257  696446.45610221]\n",
            "Epoch 877, Loss: 895585024989.6721\n",
            "Optimal weights using Gradient Descent: [4766729.21742921  821968.74269612  300260.3948257   696446.46418846]\n",
            "Epoch 878, Loss: 895585024989.6597\n",
            "Optimal weights using Gradient Descent: [4766729.21773198  821968.74180582  300260.38714714  696446.47222456]\n",
            "Epoch 879, Loss: 895585024989.6472\n",
            "Optimal weights using Gradient Descent: [4766729.21803172  821968.7409205   300260.37951657  696446.48021084]\n",
            "Epoch 880, Loss: 895585024989.6348\n",
            "Optimal weights using Gradient Descent: [4766729.21832847  821968.74004013  300260.37193371  696446.48814759]\n",
            "Epoch 881, Loss: 895585024989.6228\n",
            "Optimal weights using Gradient Descent: [4766729.21862225  821968.73916469  300260.36439825  696446.49603513]\n",
            "Epoch 882, Loss: 895585024989.6108\n",
            "Optimal weights using Gradient Descent: [4766729.21891309  821968.73829415  300260.35690988  696446.50387376]\n",
            "Epoch 883, Loss: 895585024989.599\n",
            "Optimal weights using Gradient Descent: [4766729.21920102  821968.73742849  300260.34946833  696446.51166377]\n",
            "Epoch 884, Loss: 895585024989.5873\n",
            "Optimal weights using Gradient Descent: [4766729.21948608  821968.73656767  300260.3420733   696446.51940548]\n",
            "Epoch 885, Loss: 895585024989.5758\n",
            "Optimal weights using Gradient Descent: [4766729.21976828  821968.73571168  300260.33472448  696446.52709919]\n",
            "Epoch 886, Loss: 895585024989.5645\n",
            "Optimal weights using Gradient Descent: [4766729.22004766  821968.73486049  300260.32742161  696446.53474518]\n",
            "Epoch 887, Loss: 895585024989.5532\n",
            "Optimal weights using Gradient Descent: [4766729.22032425  821968.73401407  300260.32016437  696446.54234375]\n",
            "Epoch 888, Loss: 895585024989.5421\n",
            "Optimal weights using Gradient Descent: [4766729.22059807  821968.7331724   300260.3129525   696446.54989521]\n",
            "Epoch 889, Loss: 895585024989.5312\n",
            "Optimal weights using Gradient Descent: [4766729.22086916  821968.73233545  300260.30578571  696446.55739983]\n",
            "Epoch 890, Loss: 895585024989.5203\n",
            "Optimal weights using Gradient Descent: [4766729.22113753  821968.7315032   300260.29866371  696446.56485792]\n",
            "Epoch 891, Loss: 895585024989.5098\n",
            "Optimal weights using Gradient Descent: [4766729.22140322  821968.73067563  300260.29158623  696446.57226975]\n",
            "Epoch 892, Loss: 895585024989.4991\n",
            "Optimal weights using Gradient Descent: [4766729.22166625  821968.72985271  300260.28455298  696446.57963562]\n",
            "Epoch 893, Loss: 895585024989.4885\n",
            "Optimal weights using Gradient Descent: [4766729.22192665  821968.72903441  300260.27756368  696446.58695581]\n",
            "Epoch 894, Loss: 895585024989.4785\n",
            "Optimal weights using Gradient Descent: [4766729.22218445  821968.72822071  300260.27061808  696446.5942306 ]\n",
            "Epoch 895, Loss: 895585024989.4684\n",
            "Optimal weights using Gradient Descent: [4766729.22243967  821968.72741159  300260.26371588  696446.60146027]\n",
            "Epoch 896, Loss: 895585024989.4583\n",
            "Optimal weights using Gradient Descent: [4766729.22269234  821968.72660702  300260.25685681  696446.60864511]\n",
            "Epoch 897, Loss: 895585024989.4485\n",
            "Optimal weights using Gradient Descent: [4766729.22294248  821968.72580697  300260.25004062  696446.61578539]\n",
            "Epoch 898, Loss: 895585024989.4386\n",
            "Optimal weights using Gradient Descent: [4766729.22319012  821968.72501143  300260.24326702  696446.62288138]\n",
            "Epoch 899, Loss: 895585024989.429\n",
            "Optimal weights using Gradient Descent: [4766729.22343528  821968.72422037  300260.23653576  696446.62993337]\n",
            "Epoch 900, Loss: 895585024989.4194\n",
            "Optimal weights using Gradient Descent: [4766729.22367799  821968.72343377  300260.22984656  696446.63694162]\n",
            "Epoch 901, Loss: 895585024989.4099\n",
            "Optimal weights using Gradient Descent: [4766729.22391828  821968.7226516   300260.22319917  696446.64390641]\n",
            "Epoch 902, Loss: 895585024989.4006\n",
            "Optimal weights using Gradient Descent: [4766729.22415616  821968.72187384  300260.21659332  696446.650828  ]\n",
            "Epoch 903, Loss: 895585024989.3915\n",
            "Optimal weights using Gradient Descent: [4766729.22439166  821968.72110046  300260.21002875  696446.65770666]\n",
            "Epoch 904, Loss: 895585024989.3824\n",
            "Optimal weights using Gradient Descent: [4766729.22462481  821968.72033145  300260.2035052   696446.66454266]\n",
            "Epoch 905, Loss: 895585024989.3734\n",
            "Optimal weights using Gradient Descent: [4766729.22485562  821968.71956678  300260.19702242  696446.67133626]\n",
            "Epoch 906, Loss: 895585024989.3645\n",
            "Optimal weights using Gradient Descent: [4766729.22508413  821968.71880642  300260.19058016  696446.67808773]\n",
            "Epoch 907, Loss: 895585024989.3558\n",
            "Optimal weights using Gradient Descent: [4766729.22531035  821968.71805036  300260.18417815  696446.68479732]\n",
            "Epoch 908, Loss: 895585024989.3473\n",
            "Optimal weights using Gradient Descent: [4766729.22553432  821968.71729856  300260.17781614  696446.6914653 ]\n",
            "Epoch 909, Loss: 895585024989.3387\n",
            "Optimal weights using Gradient Descent: [4766729.22575604  821968.71655101  300260.17149389  696446.69809192]\n",
            "Epoch 910, Loss: 895585024989.3302\n",
            "Optimal weights using Gradient Descent: [4766729.22597554  821968.71580769  300260.16521115  696446.70467744]\n",
            "Epoch 911, Loss: 895585024989.3219\n",
            "Optimal weights using Gradient Descent: [4766729.22619285  821968.71506857  300260.15896766  696446.71122211]\n",
            "Epoch 912, Loss: 895585024989.3138\n",
            "Optimal weights using Gradient Descent: [4766729.22640798  821968.71433363  300260.15276319  696446.71772619]\n",
            "Epoch 913, Loss: 895585024989.3055\n",
            "Optimal weights using Gradient Descent: [4766729.22662097  821968.71360285  300260.14659749  696446.72418993]\n",
            "Epoch 914, Loss: 895585024989.2975\n",
            "Optimal weights using Gradient Descent: [4766729.22683182  821968.71287621  300260.14047031  696446.73061357]\n",
            "Epoch 915, Loss: 895585024989.2896\n",
            "Optimal weights using Gradient Descent: [4766729.22704057  821968.71215367  300260.13438142  696446.73699737]\n",
            "Epoch 916, Loss: 895585024989.2819\n",
            "Optimal weights using Gradient Descent: [4766729.22724723  821968.71143523  300260.12833057  696446.74334158]\n",
            "Epoch 917, Loss: 895585024989.2742\n",
            "Optimal weights using Gradient Descent: [4766729.22745182  821968.71072086  300260.12231753  696446.74964643]\n",
            "Epoch 918, Loss: 895585024989.2665\n",
            "Optimal weights using Gradient Descent: [4766729.22765436  821968.71001053  300260.11634206  696446.75591218]\n",
            "Epoch 919, Loss: 895585024989.2588\n",
            "Optimal weights using Gradient Descent: [4766729.22785489  821968.70930423  300260.11040393  696446.76213906]\n",
            "Epoch 920, Loss: 895585024989.2515\n",
            "Optimal weights using Gradient Descent: [4766729.2280534   821968.70860194  300260.10450289  696446.76832732]\n",
            "Epoch 921, Loss: 895585024989.2441\n",
            "Optimal weights using Gradient Descent: [4766729.22824993  821968.70790363  300260.09863873  696446.77447719]\n",
            "Epoch 922, Loss: 895585024989.2369\n",
            "Optimal weights using Gradient Descent: [4766729.2284445   821968.70720927  300260.0928112   696446.78058892]\n",
            "Epoch 923, Loss: 895585024989.2297\n",
            "Optimal weights using Gradient Descent: [4766729.22863711  821968.70651886  300260.08702009  696446.78666273]\n",
            "Epoch 924, Loss: 895585024989.2225\n",
            "Optimal weights using Gradient Descent: [4766729.22882781  821968.70583237  300260.08126515  696446.79269887]\n",
            "Epoch 925, Loss: 895585024989.2158\n",
            "Optimal weights using Gradient Descent: [4766729.22901659  821968.70514978  300260.07554617  696446.79869756]\n",
            "Epoch 926, Loss: 895585024989.2087\n",
            "Optimal weights using Gradient Descent: [4766729.22920349  821968.70447106  300260.06986292  696446.80465905]\n",
            "Epoch 927, Loss: 895585024989.2021\n",
            "Optimal weights using Gradient Descent: [4766729.22938852  821968.7037962   300260.06421518  696446.81058355]\n",
            "Epoch 928, Loss: 895585024989.1952\n",
            "Optimal weights using Gradient Descent: [4766729.2295717   821968.70312518  300260.05860272  696446.81647131]\n",
            "Epoch 929, Loss: 895585024989.1886\n",
            "Optimal weights using Gradient Descent: [4766729.22975305  821968.70245797  300260.05302532  696446.82232254]\n",
            "Epoch 930, Loss: 895585024989.1819\n",
            "Optimal weights using Gradient Descent: [4766729.22993258  821968.70179455  300260.04748277  696446.82813747]\n",
            "Epoch 931, Loss: 895585024989.1757\n",
            "Optimal weights using Gradient Descent: [4766729.23011032  821968.7011349   300260.04197484  696446.83391634]\n",
            "Epoch 932, Loss: 895585024989.1691\n",
            "Optimal weights using Gradient Descent: [4766729.23028628  821968.70047902  300260.03650132  696446.83965935]\n",
            "Epoch 933, Loss: 895585024989.1627\n",
            "Optimal weights using Gradient Descent: [4766729.23046048  821968.69982686  300260.031062    696446.84536674]\n",
            "Epoch 934, Loss: 895585024989.1567\n",
            "Optimal weights using Gradient Descent: [4766729.23063294  821968.69917841  300260.02565665  696446.85103872]\n",
            "Epoch 935, Loss: 895585024989.1505\n",
            "Optimal weights using Gradient Descent: [4766729.23080368  821968.69853366  300260.02028507  696446.85667551]\n",
            "Epoch 936, Loss: 895585024989.1443\n",
            "Optimal weights using Gradient Descent: [4766729.2309727   821968.69789258  300260.01494705  696446.86227734]\n",
            "Epoch 937, Loss: 895585024989.1383\n",
            "Optimal weights using Gradient Descent: [4766729.23114004  821968.69725516  300260.00964237  696446.86784442]\n",
            "Epoch 938, Loss: 895585024989.1326\n",
            "Optimal weights using Gradient Descent: [4766729.23130571  821968.69662136  300260.00437083  696446.87337696]\n",
            "Epoch 939, Loss: 895585024989.1266\n",
            "Optimal weights using Gradient Descent: [4766729.23146971  821968.69599119  300259.99913222  696446.87887518]\n",
            "Epoch 940, Loss: 895585024989.121\n",
            "Optimal weights using Gradient Descent: [4766729.23163208  821968.6953646   300259.99392633  696446.88433929]\n",
            "Epoch 941, Loss: 895585024989.1151\n",
            "Optimal weights using Gradient Descent: [4766729.23179282  821968.69474159  300259.98875296  696446.8897695 ]\n",
            "Epoch 942, Loss: 895585024989.1095\n",
            "Optimal weights using Gradient Descent: [4766729.23195196  821968.69412213  300259.98361191  696446.89516602]\n",
            "Epoch 943, Loss: 895585024989.1039\n",
            "Optimal weights using Gradient Descent: [4766729.2321095   821968.69350621  300259.97850297  696446.90052906]\n",
            "Epoch 944, Loss: 895585024989.0983\n",
            "Optimal weights using Gradient Descent: [4766729.23226547  821968.69289381  300259.97342594  696446.90585884]\n",
            "Epoch 945, Loss: 895585024989.0929\n",
            "Optimal weights using Gradient Descent: [4766729.23241988  821968.6922849   300259.96838062  696446.91115554]\n",
            "Epoch 946, Loss: 895585024989.0874\n",
            "Optimal weights using Gradient Descent: [4766729.23257275  821968.69167947  300259.96336682  696446.91641939]\n",
            "Epoch 947, Loss: 895585024989.0822\n",
            "Optimal weights using Gradient Descent: [4766729.23272408  821968.6910775   300259.95838433  696446.92165058]\n",
            "Epoch 948, Loss: 895585024989.077\n",
            "Optimal weights using Gradient Descent: [4766729.23287391  821968.69047898  300259.95343297  696446.92684932]\n",
            "Epoch 949, Loss: 895585024989.0719\n",
            "Optimal weights using Gradient Descent: [4766729.23302223  821968.68988387  300259.94851253  696446.9320158 ]\n",
            "Epoch 950, Loss: 895585024989.0667\n",
            "Optimal weights using Gradient Descent: [4766729.23316908  821968.68929217  300259.94362282  696446.93715023]\n",
            "Epoch 951, Loss: 895585024989.0616\n",
            "Optimal weights using Gradient Descent: [4766729.23331445  821968.68870385  300259.93876365  696446.9422528 ]\n",
            "Epoch 952, Loss: 895585024989.0566\n",
            "Optimal weights using Gradient Descent: [4766729.23345837  821968.6881189   300259.93393484  696446.94732372]\n",
            "Epoch 953, Loss: 895585024989.0516\n",
            "Optimal weights using Gradient Descent: [4766729.23360085  821968.68753729  300259.92913618  696446.95236317]\n",
            "Epoch 954, Loss: 895585024989.0469\n",
            "Optimal weights using Gradient Descent: [4766729.2337419   821968.68695902  300259.92436749  696446.95737136]\n",
            "Epoch 955, Loss: 895585024989.042\n",
            "Optimal weights using Gradient Descent: [4766729.23388155  821968.68638405  300259.91962859  696446.96234848]\n",
            "Epoch 956, Loss: 895585024989.0372\n",
            "Optimal weights using Gradient Descent: [4766729.2340198   821968.68581238  300259.91491928  696446.96729471]\n",
            "Epoch 957, Loss: 895585024989.0325\n",
            "Optimal weights using Gradient Descent: [4766729.23415666  821968.68524399  300259.91023938  696446.97221026]\n",
            "Epoch 958, Loss: 895585024989.028\n",
            "Optimal weights using Gradient Descent: [4766729.23429216  821968.68467885  300259.90558872  696446.97709531]\n",
            "Epoch 959, Loss: 895585024989.0233\n",
            "Optimal weights using Gradient Descent: [4766729.2344263   821968.68411696  300259.90096709  696446.98195005]\n",
            "Epoch 960, Loss: 895585024989.0189\n",
            "Optimal weights using Gradient Descent: [4766729.23455911  821968.68355828  300259.89637433  696446.98677467]\n",
            "Epoch 961, Loss: 895585024989.0145\n",
            "Optimal weights using Gradient Descent: [4766729.23469058  821968.68300281  300259.89181026  696446.99156936]\n",
            "Epoch 962, Loss: 895585024989.0101\n",
            "Optimal weights using Gradient Descent: [4766729.23482074  821968.68245053  300259.88727468  696446.99633429]\n",
            "Epoch 963, Loss: 895585024989.0056\n",
            "Optimal weights using Gradient Descent: [4766729.23494959  821968.68190141  300259.88276743  696447.00106966]\n",
            "Epoch 964, Loss: 895585024989.0015\n",
            "Optimal weights using Gradient Descent: [4766729.23507716  821968.68135545  300259.87828833  696447.00577565]\n",
            "Epoch 965, Loss: 895585024988.9972\n",
            "Optimal weights using Gradient Descent: [4766729.23520346  821968.68081262  300259.8738372   696447.01045244]\n",
            "Epoch 966, Loss: 895585024988.993\n",
            "Optimal weights using Gradient Descent: [4766729.23532849  821968.68027291  300259.86941387  696447.01510022]\n",
            "Epoch 967, Loss: 895585024988.9888\n",
            "Optimal weights using Gradient Descent: [4766729.23545226  821968.6797363   300259.86501815  696447.01971915]\n",
            "Epoch 968, Loss: 895585024988.9847\n",
            "Optimal weights using Gradient Descent: [4766729.23557481  821968.67920277  300259.86064989  696447.02430942]\n",
            "Epoch 969, Loss: 895585024988.9808\n",
            "Optimal weights using Gradient Descent: [4766729.23569612  821968.67867231  300259.85630891  696447.02887121]\n",
            "Epoch 970, Loss: 895585024988.9768\n",
            "Optimal weights using Gradient Descent: [4766729.23581623  821968.6781449   300259.85199504  696447.0334047 ]\n",
            "Epoch 971, Loss: 895585024988.9729\n",
            "Optimal weights using Gradient Descent: [4766729.23593513  821968.67762052  300259.8477081   696447.03791006]\n",
            "Epoch 972, Loss: 895585024988.9689\n",
            "Optimal weights using Gradient Descent: [4766729.23605284  821968.67709916  300259.84344793  696447.04238746]\n",
            "Epoch 973, Loss: 895585024988.9652\n",
            "Optimal weights using Gradient Descent: [4766729.23616938  821968.67658079  300259.83921437  696447.04683708]\n",
            "Epoch 974, Loss: 895585024988.9613\n",
            "Optimal weights using Gradient Descent: [4766729.23628475  821968.67606541  300259.83500724  696447.05125908]\n",
            "Epoch 975, Loss: 895585024988.9576\n",
            "Optimal weights using Gradient Descent: [4766729.23639896  821968.675553    300259.83082638  696447.05565365]\n",
            "Epoch 976, Loss: 895585024988.9539\n",
            "Optimal weights using Gradient Descent: [4766729.23651204  821968.67504354  300259.82667162  696447.06002095]\n",
            "Epoch 977, Loss: 895585024988.9501\n",
            "Optimal weights using Gradient Descent: [4766729.23662398  821968.67453701  300259.82254281  696447.06436115]\n",
            "Epoch 978, Loss: 895585024988.9467\n",
            "Optimal weights using Gradient Descent: [4766729.23673481  821968.6740334   300259.81843978  696447.06867442]\n",
            "Epoch 979, Loss: 895585024988.9431\n",
            "Optimal weights using Gradient Descent: [4766729.23684452  821968.67353269  300259.81436237  696447.07296093]\n",
            "Epoch 980, Loss: 895585024988.9395\n",
            "Optimal weights using Gradient Descent: [4766729.23695314  821968.67303486  300259.81031042  696447.07722083]\n",
            "Epoch 981, Loss: 895585024988.9359\n",
            "Optimal weights using Gradient Descent: [4766729.23706067  821968.67253991  300259.80628377  696447.0814543 ]\n",
            "Epoch 982, Loss: 895585024988.9326\n",
            "Optimal weights using Gradient Descent: [4766729.23716713  821968.6720478   300259.80228226  696447.0856615 ]\n",
            "Epoch 983, Loss: 895585024988.9293\n",
            "Optimal weights using Gradient Descent: [4766729.23727252  821968.67155854  300259.79830574  696447.08984259]\n",
            "Epoch 984, Loss: 895585024988.9259\n",
            "Optimal weights using Gradient Descent: [4766729.23737686  821968.6710721   300259.79435404  696447.09399774]\n",
            "Epoch 985, Loss: 895585024988.9226\n",
            "Optimal weights using Gradient Descent: [4766729.23748016  821968.67058847  300259.79042702  696447.0981271 ]\n",
            "Epoch 986, Loss: 895585024988.9193\n",
            "Optimal weights using Gradient Descent: [4766729.23758242  821968.67010762  300259.78652451  696447.10223084]\n",
            "Epoch 987, Loss: 895585024988.9161\n",
            "Optimal weights using Gradient Descent: [4766729.23768366  821968.66962956  300259.78264637  696447.10630911]\n",
            "Epoch 988, Loss: 895585024988.9131\n",
            "Optimal weights using Gradient Descent: [4766729.23778389  821968.66915425  300259.77879245  696447.11036208]\n",
            "Epoch 989, Loss: 895585024988.9098\n",
            "Optimal weights using Gradient Descent: [4766729.23788312  821968.66868169  300259.77496258  696447.11438989]\n",
            "Epoch 990, Loss: 895585024988.9067\n",
            "Optimal weights using Gradient Descent: [4766729.23798135  821968.66821186  300259.77115663  696447.11839271]\n",
            "Epoch 991, Loss: 895585024988.9037\n",
            "Optimal weights using Gradient Descent: [4766729.2380786   821968.66774474  300259.76737443  696447.12237068]\n",
            "Epoch 992, Loss: 895585024988.9008\n",
            "Optimal weights using Gradient Descent: [4766729.23817488  821968.66728032  300259.76361585  696447.12632397]\n",
            "Epoch 993, Loss: 895585024988.8977\n",
            "Optimal weights using Gradient Descent: [4766729.23827019  821968.66681859  300259.75988074  696447.13025273]\n",
            "Epoch 994, Loss: 895585024988.8948\n",
            "Optimal weights using Gradient Descent: [4766729.23836455  821968.66635953  300259.75616894  696447.13415711]\n",
            "Epoch 995, Loss: 895585024988.8918\n",
            "Optimal weights using Gradient Descent: [4766729.23845797  821968.66590312  300259.75248032  696447.13803726]\n",
            "Epoch 996, Loss: 895585024988.8888\n",
            "Optimal weights using Gradient Descent: [4766729.23855046  821968.66544935  300259.74881473  696447.14189332]\n",
            "Epoch 997, Loss: 895585024988.8861\n",
            "Optimal weights using Gradient Descent: [4766729.23864202  821968.66499821  300259.74517202  696447.14572546]\n",
            "Epoch 998, Loss: 895585024988.8832\n",
            "Optimal weights using Gradient Descent: [4766729.23873266  821968.66454968  300259.74155205  696447.14953381]\n",
            "Epoch 999, Loss: 895585024988.8805\n",
            "Optimal weights using Gradient Descent: [4766729.2388224   821968.66410375  300259.73795468  696447.15331853]\n",
            "Epoch 1000, Loss: 895585024988.8778\n",
            "Optimal weights using Gradient Descent: [4766729.23891124  821968.66366039  300259.73437976  696447.15707976]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Знайдіть ці ж параметри за допомогою аналітичного рішення;"
      ],
      "metadata": {
        "id": "pxsV4vV8qghd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analytical_solution(X, y):\n",
        "    return np.linalg.inv(X.T @ X) @ X.T @ y"
      ],
      "metadata": {
        "id": "zqvl0Mf9qjmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analytical_solution = analytical_solution(X_with_intercept, y)\n",
        "analytical_solution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf5T3noyqoHq",
        "outputId": "0efebf6f-62dd-4132-cb1c-01519c4cf5d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4766729.24770642,  821968.58935343,  300259.16468032,\n",
              "        696447.75898578])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Порівняйте отримані результати."
      ],
      "metadata": {
        "id": "T3xAMmXyq3-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(optimal_w)\n",
        "print(analytical_solution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc-WmSGbrA9l",
        "outputId": "a5827ae3-8f6b-455c-990a-29670ebbb6db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4766525.51991521  821953.49275582  300573.87267281  696153.45069131]\n",
            "[4766729.24770642  821968.58935343  300259.16468032  696447.75898578]\n"
          ]
        }
      ]
    }
  ]
}